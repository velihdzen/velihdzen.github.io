{"meta":{"title":"Velih Dzen's Blog","subtitle":"","description":"","author":"Velih Dzen","url":"https://velih.de","root":"/"},"pages":[{"title":"关于 Velih Dzen","date":"2020-12-04T09:53:58.996Z","updated":"2020-12-04T09:53:58.996Z","comments":true,"path":"about/index.html","permalink":"https://velih.de/about/index.html","excerpt":"","text":"Velih Dzen，数字游民一名。建此博客以分享技术与表达自我。如果有任何问题，欢迎电邮 velih.dzen@gmail.com。除此之外，还可以在这些地方找到我： GitHub: VelihDzen Twitter: LostZZWooo"},{"title":"tags","date":"2018-04-04T14:58:50.000Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"tags/index.html","permalink":"https://velih.de/tags/index.html","excerpt":"","text":""},{"title":"","date":"2020-08-23T14:04:07.917Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"downloads/code/utp_header_format.html","permalink":"https://velih.de/downloads/code/utp_header_format.html","excerpt":"","text":"Offsets Octet 0 1 2 3 Octet .mw-parser-output .monospaced { font-family: monospace, monospace } .mw-parser-output .monospaced-bold { font-weight: bold } Bit &nbsp;0 &nbsp;1 &nbsp;2 &nbsp;3 &nbsp;4 &nbsp;5 &nbsp;6 &nbsp;7 &nbsp;8 &nbsp;9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 0 &nbsp;0 Source port Destination port 4 32 Length Checksum 8 64 type ver extension connection_id 12 96 timestamp_microseconds 16 128 timestamp_difference_microseconds 20 160 wnd_size 24 192 seq_nr ack_nr 28... 224... extensions..."}],"posts":[{"title":"深圳大鹏半岛海柴角地名小考","slug":"haichaicape","date":"2022-06-30T06:09:50.000Z","updated":"2022-07-03T10:11:32.800Z","comments":true,"path":"2022/06/30/haichaicape/","link":"","permalink":"https://velih.de/2022/06/30/haichaicape/","excerpt":"","text":"居住在深圳大概率会听说过东西涌、杨梅坑、鹿嘴山庄这几个大鹏半岛上的出行与团建胜地，在东涌东北，鹿嘴山庄东南，有个现名为海柴角(位置：7PJPGJ5G+VW)的岬角，这里是深圳陆地的的最东端，深圳每天清晨的第一缕阳光最先抵达这里。 2020 年底，我斗胆只身前往从杨梅坑出发，想途径鹿嘴、海柴角徒步至东涌，当到达海柴角时天色已黑，于是就地躺在被海水冲刷亿万年的巨形火山岩上听了一晚的惊涛骇浪，有如 1987年版《深圳市地名志》所述 此角无风三尺浪，有风浪头百丈高，惊涛拍岸似劈柴声。次日清晨小雨没有看到日出，雨停后寻找穿过海柴角的山路失败，只能原路返回。这算是我第一次和海柴角相遇，想来有些后怕，海柴角所在的这段海岸线尚未开发，徒步路途漫长，山路难寻，没有补给，救援困难，个人前往一定谨慎。 前几日在《向深圳学习》一书上看到 1819 年嘉庆《新安县志》中的新安县地图，地图右侧标注有 崖柴角，崖柴角很可能就是现在的海柴角，于是有了一些好奇心来探索海柴角地名的变化。 海柴角地名变化古名 崖柴角 （1731）1819 年嘉庆《新安县志》的舆图中，崖柴角位于大鵬城（今大鹏所城）的东南方向，老大鹏（今七娘山）的东面，与现在海柴角与两地的相对位置相符，但地图中标记的三管筆如果是现在的三管笔，那其所在的大笔架山相对大鹏所城的位置与现在比是南北颠倒了。再看到另外一些地点的相对大小与相对位置都有所问题，比如深圳与黃貝嶺的相对位置，因此地图的位置信息只能做个粗略参考。 再往前追溯，在更早的 1731 年雍正《广东通志》中也有标有崖柴角的舆图。只是此图比例尺更小，出现的地名间的相对位置更乱。 别名 鞋柴角 （1983）第一次看到鞋柴角这个地名是前往海柴角徒步路线的攻略地图上，了解点粤语的人应该能很快意识到“鞋”和“海”发音上的关系，海的普通话发音 hǎi 和鞋的粤语发音 haai4 除了声调外基本上是一样的，也就是当一个只会说普通话的人去记录一个普通话不利索的粤语母语者说“鞋柴角”三个字时很可能记录成“海柴角”。现在鞋柴角地名仍然用在很多非官方地图上。比如： OpenStreetMap：节点：鞋柴角 (5626153683) Bing Map：鞋柴角 Google Map：鞋柴角 与海柴角两个地名重复并存 腾讯地图：鞋柴角 与海柴角两个地名重复并存 百度地图：鞋柴角 高德地图：海柴角。目前国内主流的地图只有高德地图是正确显示且仅显示海柴角名称的。 也有一些文献或者官方资料使用过鞋柴角名称： 1983 年出版的《深圳地貌》第 3 页记录深圳市经纬度范围时提到： 深圳市的经纬度位置，按陆地计，南起北纬22°26′59″（大鹏半岛南端），北至北纬22°51′49″（罗田水库北缘），西起东经113°45′44″（沙井均益围），东至东经114°37′2″（大鹏半岛 鞋柴角）；按岛屿计，最南点为北纬22°24′01″（内伶仃岛东角咀）；最西点为东经113°46′50″（内伶仃岛牛利角）；按海界计，南起北纬22°09′，西起东经113°9′36″，东至东经114°8′43″。 1999 年出版的《广东省志：地名志》第 515 页介绍七娘山时提到： 七娘山又名大鹏山。在宝安县东部，大鹏半岛南段的南澳镇境内。东起 鞋柴角，西至枫木浪水库，南起东角，北至东山。面积约45平方公里。主峰海拔867.4米，是宝安县第一高峰。该山7个山峰，相传是七仙女下凡所成，故名。 2014 年出版的《广东省志（1979-2000）：行政区划·地名卷》介绍七娘山时虽然更新了部分地名与描述，但是仍然在使用 鞋柴角，第 82 页： 七娘山又名大鹏山。在宝安区东部、大鹏半岛南段的南澳镇境内，东起 鞋柴角，西至枫木浪水库，南起东角，北至东山。东西长约9公里，南北宽约7公里。面积约45平方公里。主峰海拔867.4米，是宝安区第一高峰。 现名 海柴角 （1987）依据中国国家地名信息库，海柴角是现在的标准名称，可惜海柴角的地名来历、地名含义与历史沿革项均为资料暂缺状态。附近的一些地点倒是有记录，比如： 马料河：因此处经常牧马周围草料丰富且有小河，故名。 望郎归：相传马料河的村民（刚结婚不久的新娘）天天到山顶等待出海打渔的丈夫归来，不幸被山中的野猪吃掉，人们为纪念多情的新娘，故名。古名沿用至今。 鹿咀：因山形如鹿嘴，故名。“咀”同“嘴”。 1987 年出版的《深圳市地名志》中的记录算是我有限的线上检索能力能查到的最早使用海柴角作为地名的资料： 海柴角在大鹏区三门岛北面 4.3 公里，突出海面 0.55 公里，植被较好，由花岗岩构成，因此角无风三尺浪，有风浪头百丈高，惊涛拍岸似劈柴声而得名。 1988 年（在 1988-1990 年之间，具体出版年份不详）由深圳市国土局编写出版的《深圳市国土规划》甚至同时使用了鞋柴角和海柴角两个名称： 在第 1 页使用 海柴角 名称记录深圳市地理位置： 深圳市位于北纬22°27′（大鹏半岛南端）──22°52′（罗田水库北缘），东经113°46′（沙井均益围）──114°37′（大鹏半岛海柴角）。 在第 72 页使用 鞋柴角 名称记录深圳东部海岸线范围： 东部海岸线从沙头角至与惠阳交界的白沙湾总长约155公里,其中特区岸线21.4公里。主要由大鹏湾和大亚湾组成，其中大鹏湾岸线长约60公里，大鹏半岛端部（穿岩至鞋柴角）岸线24公里，大亚湾海岸线71公里。 地名管理可是现在深圳市规划和自然资源局（前生就是国土局）的业务之一！ 1997 年的《宝安县志》写到： 宝安县地形呈东西长，南北窄的狭长形地带，东端大鹏半岛海柴角至西侧福永下十围涌口82公里，南北最宽处(新安安乐村海边至松岗罗田水库北缘)34公里，最窄处(布吉沙湾圩至东莞雁田水库南缘)5.3公里。 另外 1997《宝安县志》、2012《龙岗区志（1993-2003）》、2014《深圳市志·基础建设卷》在列举区划内的岬角时都使用 海柴角。 总结大鹏半岛的居民在新中国成立前后是以广府与客家移民居多，八十年代经济特区成立后许多使用普通话的人群涌入深圳。崖（粤语：ngaai4，客家：ngai2），鞋 (粤语：haai4，客家：hai2），海（普通话：hǎi）这几个字的变化似乎也于移民使用的语言有关。写这篇小记仅是出于兴趣，由于没有专业的语言学历史学人类学知识，对于海柴角地名的历史变化也就只能做些简单的 Proof of Work 以及猜测。ai柴角第一个字在记录上有所演变，第二个字却始终刻画着它惊涛拍岸的劈柴巨响。 相关资料 史志 1688 康熙《新安县志》 1819 嘉庆《新安县志》 1731 雍正《广东通志》 1987《深圳市地名志》 1997《宝安县志》 2012《龙岗区志（1993-2003）》 2014《深圳市志·基础建设卷》 1999《广东省志：地名志》 2014《广东省志（1979-2000）：行政区划·地名卷》 史志资料 2006《康熙新安縣志校注》 2006《嘉慶新安縣志校注》 1988《深圳市国土规划》 1998 马金科《早期香港史研究資料選輯》 其他 1983 黄镇国等《深圳地貌》 2020 马立安等《向深圳学习》 在线公开数据库 中国地方志数据库 广东省情数据库 深圳史志网 - 数字资源 中国国家地名信息库 中國哲學書電子化計劃 中古擬音查詢 汉语方言发音词典","categories":[],"tags":[{"name":"考","slug":"考","permalink":"https://velih.de/tags/%E8%80%83/"}]},{"title":"BT 增强建议之进阶改进方案","slug":"bt-advanced","date":"2018-11-15T11:45:00.000Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"2018/11/15/bt-advanced/","link":"","permalink":"https://velih.de/2018/11/15/bt-advanced/","excerpt":"","text":"本文是 BT 系列文章中的一篇，主要介绍 BEP 中剩下的改进方案，有需要的话可以先阅读博文 BT 增强建议之概述。 进阶改进方案本节点详细介绍几个比较重要的进阶改进方案。剩余的只作简单归纳。 超级做种模式提高做种效率BEP16 - Superseeding 中起草的超级做种功能是一个来帮助初始做种者使用较少的流量来完成做种的算法。当一个做种客户端进入超级做种模式后，它将不会表现为一个标准的做种者，而是伪装成一个没有数据的正常 peer。当有其他 peer 连接时，它仅将一个从未发送过的片段发送给刚连接的 peer，在一个别的 peer 通知到该做种者这个片段前，做种者不会向刚才的 peer 发送新的片段，以此节约做种者的流量。一般情况下，不建议使用超级种子模式。虽然它确实有助于更广泛地分发稀有数据，因为它限制了客户端可以下载的片段的选择，但也限制了这些客户端下载已经检索到的部分片段数据的能力。因此，超级种子模式仅推荐用于初始种子服务器。 使用 Web 方式做种WebSeed 在 BEP 中有两个增强建议：BEP19 - HTTP&#x2F;FTP Seeding (GetRight style) 和 BEP17 - Seeding (Hoffman-style) BEP17 提供了一种通过 HTTP 协议做种以及从 HTTP 获取数据的方式。初始做种者主要在元数据文件中加入键 httpseeds，值为支持 HTTP Seeding 服务器地址列表，举例如下： 12d[&#x27;httpseeds&#x27;] = [ &#x27;http://www.site1.com/source1.php&#x27;, &#x27;http://www.site2.com/source2.php&#x27; ] 客户端通过以下方式访问 HTTP Seeding 服务器获取全部或者特定数据片段。 1&lt;url&gt;?info_hash=[hash]&amp;piece=[piece]&#123;&amp;ranges=[start]-[end]&#123;,[start]-[end]&#125;...&#125; BEP19 通告服务器地址的方式和 BEP17 差不多，但是键改为了 url-list。BEP19 的优点是能够处理任何可通过普通 HTTP 或 FTP 访问的文件，利用标准 HTTP 请求下载片段。BEP17 要求对服务器做适当修改，以支持 BEP17，这在互操作性方面是一个缺点，但它确实具有允许种子服务器更好地控制流量和避免带宽滥用的优势。 私有种子加强数据分享隐私性私有种子是在 BEP27 - Private Torrents 中描述的，通过在元数据信息总增加键值对 private=1 来标记种子为私有种子。客户端只能向私有种子对应的私有 Tracker 通告自身，而且只能与私有 Tracker 返回的 peer 建立连接。 当私有种子的 announce-list 中出现多个 Tracker 地址时，每个 peer 在一个时刻只能使用其中一个 Tracker，只有当 Tracker 出错时才可以切换 Tracker。在切换 Tracker 时，peer 必须断开所有与其他 peer 的连接，之后只能与新的 Tracker 提供的 peer 建立连接。这减轻了攻击者通过修改元数据文件中 announce-list 后发布新的元数据文件进行攻击带来的影响。 当种子为私有种子时，通过其他方式（包括 DHT, PEX, LSD 等）获取 peer 都会对“私有”进行破坏。但是不能避免攻击者通过猜测 peer 的 IP 与端口的方式找到 peer，进而产生连接。 Merkle Tree 数据结构优化元数据文件大小如果种子文件过大，对于提供种子文件下载的中心化服务器就会有较大压力。为了让元数据文件体积减小，一个有效的方法就是将每个片段的大小增加（上限是 2 Mb），同样大小的文件因此会产生更少的摘要数量。考虑到只有当一个片段被完全接受完成且验证摘要后，才能将之与其他 peer 进行交换，这意味着节点需要一段时间才能和其他 peer 进行交换。 BEP30 - Merkle hash torrent extension 利用了 Merkle Tree 这种数据结构来优化元数据文件大小。该方案使用单一的 Merkle 哈希来替代摘要列表。Merkle 散列可用于通过分层方案验证整个内容文件以及各个块的完整性。它通过构建与数据相关的哈希树并仅需使用根哈希作为验证数据完整性的依据。 如图所示，P0-P12 代表片段 0-12，X 是为了树的完整而填充的零值片段，0-30 均表示哈希，其中 0 是根哈希。如果一个节点接受到了完整的片段 P8，那么该节点只需要额外通过一些途径知道 P8 的姊妹片段 P9 的哈希值，以及 P8 的所有叔块哈希（依次为 12，6，1）就可以验证 P8 的完整性。 1234567891011121314151617181920212223242526272829 0* = root hash / \\ / \\ / \\ / \\ / \\ 1* 2 / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ 3 4 5 6* = uncle / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ 7 8 9 10 11 12* 13 14 / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30P0 P1 P2 P3 P4 P5 P6 P7 P8* P9* P10 P11 P12 X X X= piece index = = = filler hash p s i i e b c l e i n g 原始资源发布者需要将元数据中的原本用来存储片段散列值的 pieces 键替换为存储根哈希的键 root hash。 支持 Merkle Tree 哈希方式的客户端需要同时支持扩展消息（见 BT 增强建议之 Peer）来替代普通的 piece 消息来完成数据传输。用于传输数据的扩展消息类型是 Tr_hashpiece ，和其他扩展消息一样，这个消息也需要在 peer 间的扩展握手消息中声明。 Tr_hashpiece 消息的负载内容如下： index：4 个字节，代表数据片段序号； begin：4 个字节，代表当前当前数据子片段在整个片段中的偏移量； hashlist length：4 个字节，代表哈希列表的长度 hashlist：哈希列表，包括片段自身哈希，姊妹片段哈希，以及各个叔片段哈希，最后是根哈希，只有当 begin 为 0 时，才需要传输 hashlist； subpiece data：数据子片段； 在 peer 接收完成某个数据片段的 Tr_hashpiece 消息后，它将根据 hashlist 中的哈希列表计算出的根哈希与原有的根哈希进行比对。如果匹配，所有哈希值都保存在该 peer 自己的 Merkle Tree 中，之后它们可以被传递给其他人。 需要注意的是，使用 Merkle Tree 的 Torrent 与使用散列值列表的 Torrent 即使它们的数据完全相同，但是由于 infohash 不一致，导致它们不在同一个 Swarm 中。另外一旦采用了 Merkle Tree 的方式，通过 WebSeed 来获取数据就变得不兼容。 其他方案简述 BEP35 - Torrent Signing 利用对种子进行签名以增加下载安全性。 BEP36 - Torrent RSS feeds 定义了 RSS 订阅种子的 feed 格式。 BEP38 - Finding Local Data Via Torrent File Hints 则提出了粗略地检测用户本地是否已经存在全部或者部分即将下载的文件的方案。主要涉及在种子文件中添加 similar 或 collections 这两个键。前者表示可能与某个 infohash 共享部分数据，后者表示该种子文件所属的集合，属于同一集合的种子可能共享文件。 BEP39 - Updating Torrents Via Feed URL 使用在 info 中的键 update-url 来表明用于更新该种子的链接地址，如果客户端请求该地址成功且得到一个种子文件，则需要下载该更新后的种子。 BEP47 - Padding files and extended file attributes 在原始元数据描述文件信息的基础上增加了一些额外的属性。例如符号链接重复文件；为文件添加 padding 以使得下一个文件数据从片段边缘开始。 BEP49 - Distributed Torrent Feeds 与 BEP36 提供的 RSS 订阅功能类似，但是通过 DHT 网络实现。 参考资料 Implement Web (HTTP) Seeding (BEP17+BEP19) · Issue #67 · webtorrent&#x2F;webtorrent","categories":[],"tags":[{"name":"BT","slug":"BT","permalink":"https://velih.de/tags/BT/"}]},{"title":"BT 增强建议之 DHT","slug":"bt-dht","date":"2018-11-12T13:11:00.000Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"2018/11/12/bt-dht/","link":"","permalink":"https://velih.de/2018/11/12/bt-dht/","excerpt":"","text":"BitTorrent 使用 DHT 网路来存储 peer 信息，以实现去 Tracker 化的种子。此时，每个 peer 都成了一个 Tracker。网络基于 Kademlia 算法实现，使用 UDP 进行传输。 本文主要对 BT 中的 DHT 网络实现与在博文 DHT 网络之 Kademlia 算法 中描述的 Kademlia 算法的区别进行总结。准确的说不能说是区别，而应该定义成实现细节，毕竟理论算法应用到实际时总需要因地制宜。 实现细节点key-value 对存储的内容BT 使用 DHT 网络来实现去 Tracker 化，因此 peer 的信息就需要存储在 DHT 网络中，BT 中 peer 又是局限在某个 Torrent 文件中的。因此 key-value 对中 key 即 Torrent 文件的 infohash，正好 infohash 也是 160 比特长度的，value 为拥有该 infohash 文件的 peer 列表。 $K$ 桶实现细节首先 $K$ 桶中的节点有多种状态：如果一个节点在 15 分钟内回复过当前的节点的查询请求或者曾经回复过当前节点的查询请求同时在 15 分钟内有发送过查询请求给当前节点，则该节点相对于当前节点为 Good 节点；如果一个节点 15 分钟内未曾活动过，则成为 Questionable 节点；如果一个节点未相应当前节点的多次查询请求，则视为 Bad 节点。 当一个桶中节点数量已满（$K$ 的容量 $k$ 在 BT 中为 8）且都是 Good 节点，新的节点如果想要加入则直接被忽略。如果桶中有 Bad 节点，新的节点将会替代之。如果桶中有 Questionable 节点，则按照这些节点加入桶的时间进行 ping 请求，如果有节点不能被 ping 成功则新的节点将会取代之。 每个桶都会有一个最近更新时间标记来指示桶中节点的新鲜程度。如果一个桶中节点被 ping 成功了，或者有新的节点加入，或者一个节点被另一个替代了，这个时间会更新。如果一个桶最近更新时间已经过去 15 分钟，则需要进行一次桶刷新，方式和 Kademlia 中描述的一致。 Peer DHT 支持与否告知peer 通过在 handshake 握手消息中将 reserved_byte 的最后一比特设置为 1 来表示支持 DHT 网络。同样支持 DHT 网络的节点收到该握手消息后会发起一个 PORT 消息，type 为 9，负载为两个字节的 DHT 节点 UDP 端口。收到 PORT 消息的节点需要对改节点端口执行 ping 操作，如果对方响应了，则节点需要尝试将其加入到 $K$ 桶中。 初始节点获取去 Tracker 化的 Torrent 文件中是没有存储 Tracker 地址的 ”announce“ 键的。取而代之的是 ”nodes“ 键，nodes 中需要存储距离 key 最近的 $k$ 个节点地址或者存储一个 Good 节点，比如说产生这个 Torrent 的用户节点。不能将某些公共的节点加入到 Torrent 文件中，否则就变得不那么去 Tracker 化了。nodes 大概可以表示为： 12nodes = [[&quot;&lt;host&gt;&quot;, &lt;port&gt;], [&quot;&lt;host&gt;&quot;, &lt;port&gt;], ...]nodes = [[&quot;127.0.0.1&quot;, 6881], [&quot;your.router.node&quot;, 4804]] RPC 消息BT 使用 KRPC 来实现 Kademlia 节点间的 RPC 通信，KRPC 消息有三种消息类型：查询，响应，错误。对于 DHT 协议，有四种查询协议：ping，find_node，get_peers，announce_peer。 KRPC 协议KRPC 协议是基于 UDP 和 bencode 编码的 RPC 协议。每条 KRPC 消息带有三个公共键值对以及其他因消息类型而异的键。三个公共的键是： ”t“：值为字符串，代表 transaction ID。由发起 query 请求的节点生成，被请求节点需要在回复中返回。一般两个字节； ”y“：值为单字符，代表消息类型，是 q(query)、r(response)、e(error) 中的一个； “v”：值为字符串，代表客户端版本 BEP 20 中定义，不一定存在这个键； 每种消息会有自己额外的键： Query 消息 ”q“：值为字符串，代表具体的方法类型； ”a“：值为字典，代表具体方法对应的参数字典； Response 消息如果 Query 消息执行成功，则会返回该 Response 消息。 ”r“：值为字典，代表返回值字典； Error 消息如果 Query 消息执行失败，则会返回该 Error 消息。 ”e“：值为列表，第一个元素代表错误码，第二个元素是错误消息； 错误消息类型如下： Code Description 201 Generic Error 202 Server Error 203 Protocol Error, such as a malformed packet, invalid arguments, or bad token 204 Method Unknown 示例错误消息： 12generic error = &#123;&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;e&quot;, &quot;e&quot;:[201, &quot;A Generic Error Ocurred&quot;]&#125;bencoded = d1:eli201e23:A Generic Error Ocurrede1:t2:aa1:y1:ee DHT Query 消息ping用于检测一个节点是否工作，相当于 Kademlia 的 PING 调用。 123arguments: &#123;&quot;id&quot; : &quot;&lt;querying nodes id&gt;&quot;&#125;response: &#123;&quot;id&quot; : &quot;&lt;queried nodes id&gt;&quot;&#125; find_node查找距离指定节点 ID 最近的节点 ID 信息，相当于 Kademlia 的 FIND_NODE 调用。 123arguments: &#123;&quot;id&quot; : &quot;&lt;querying nodes id&gt;&quot;, &quot;target&quot; : &quot;&lt;id of target node&gt;&quot;&#125;response: &#123;&quot;id&quot; : &quot;&lt;queried nodes id&gt;&quot;, &quot;nodes&quot; : &quot;&lt;compact node info&gt;&quot;&#125; “Compact node info” 列表中每个节点信息占用 26 字节，其中节点 ID 20 字节，IP 与 Port 占用 6 字节。 get_peers查找与指定 info_hash 相关的 peer 信息，相当于 Kademlia 的 FIND_VALUE 调用。 12345arguments: &#123;&quot;id&quot; : &quot;&lt;querying nodes id&gt;&quot;, &quot;info_hash&quot; : &quot;&lt;20-byte infohash of target torrent&gt;&quot;&#125;response: &#123;&quot;id&quot; : &quot;&lt;queried nodes id&gt;&quot;, &quot;token&quot; :&quot;&lt;opaque write token&gt;&quot;, &quot;values&quot; : [&quot;&lt;peer 1 info string&gt;&quot;, &quot;&lt;peer 2 info string&gt;&quot;]&#125;or: &#123;&quot;id&quot; : &quot;&lt;queried nodes id&gt;&quot;, &quot;token&quot; :&quot;&lt;opaque write token&gt;&quot;, &quot;nodes&quot; : &quot;&lt;compact node info&gt;&quot;&#125; 如果被请求节点有指定 info_hash 相关 peers 则以 values 为键的 “Compact IP-address&#x2F;port info” 列表，否则，返回以 nodes 为键的 “Compact node info” 列表。响应中的 token 需要请求者在通过 announce_peer 向回复者宣告 peer 信息时携带。 announce_peer宣告节点自身拥有特定 info_hash 的数据，并在某个端口进行下载，相当于 Kademlia 的 STORE 调用。 1234567arguments: &#123;&quot;id&quot; : &quot;&lt;querying nodes id&gt;&quot;, &quot;implied_port&quot;: &lt;0 or 1&gt;, &quot;info_hash&quot; : &quot;&lt;20-byte infohash of target torrent&gt;&quot;, &quot;port&quot; : &lt;port number&gt;, &quot;token&quot; : &quot;&lt;opaque token&gt;&quot;&#125;response: &#123;&quot;id&quot; : &quot;&lt;queried nodes id&gt;&quot;&#125; 接受者需要校验 token 是否与之前它通过 get_peers 调用的响应回复给该请求者的一致。如果 implied_port 值为 1，表示 port 的值不可用，接受者需要将请求包的 UDP 源端口作为 port。 BT DHT 相关增强方案DHT Scrape 帮助选择 Seeding 内容类似 Tracker Scrape，DHT 网络可以通过 BEP33 - DHT Scrapes 中定义的 DHT scrape 来了解某个 Swarm 中 peer 的大致情况，然后根据这个状态选择 seeding 队列中下一个 Swarm 进行做种。这种算法主要通过布隆过滤器（Bloom Filter）实现，布隆过滤器通常用于检索一个元素是否在一个集合中。但在 DHT scrape 中，作用所有不同。加入布隆过滤器中的是各个 peer 的 IP 的 sha1 值，可以根据过滤器中剩余的 0 比特数量来估算整个 Swarm 的规模。 只读 DHT 节点在一些情况下，DHT 节点主动或被动地限制成为只读节点（BEP43 - Read-only DHT Nodes 中定义），比如位于 NAT 后且 hole punching 失败的节点，节点具有流量限制或者有流量计划，流量会影响节点的电量等等情况。 节点通过在每条向外发送的 DHT Query 消息中给出一个 ro=1 的键值对来表明自己为只读节点。成为只读节点后，不再响应其他节点的 Query 请求。其他节点知晓只读节点后也不会发送 Query 请求以减少网络流量。 在 DHT 中存储任意数据BEP44 - Storing arbitrary data in the DHT 中提供了一种在 DHT 中存储任意数据的方式。存储的数据可以是不可变数据也可以是可变数据，不可变数据的 key 是数据内容的 sha1 值，可变数据的 key 是用于签名数据的密钥对公钥。BEP46 - Updating Torrents Via DHT Mutable Items 则基于在 DHT 网络中存储可变数据而给出了一种用于更新 Torrent 的方法。BEP50 - Publish&#x2F;Subscribe Protocol 实现了一种基于主题的发布订阅模式来向订阅特定主题的客户端发送更新后可变数据的协议。 多个监听地址情况处理某些客户端可能会监听多个公网单播 IP 地址，如果在这种情况下，该客户端仅使用一个节点 ID，则其他节点可能会因为多地址现象而对该节点 ID 进行清理。BEP45 - Multiple-address operation for the BitTorrent DHT 中给出了一些要求和建议： 要求每个套接字地址必须具有不同的节点 ID，且它们的 XOR 距离要分得比较开，响应必须从收到相应请求的同一套接字地址发送； 建议节点应避免在单个IP地址上使用多个端口； 其他和 DHT 相关的草案还有如对 IPv6 的支持（BEP32 - BitTorrent DHT Extensions for IPv6），DHT 网络安全（BEP42 - DHT Security extension），检索其他节点存储的 infohash 列表（BEP51 - DHT Infohash Indexing）等。 参考资料 μTorrent: What’s the difference between the status “Queued Seed” and “Seeding”?","categories":[],"tags":[{"name":"BT","slug":"BT","permalink":"https://velih.de/tags/BT/"}]},{"title":"DHT 网络之 Kademlia 算法","slug":"dht-kademlia","date":"2018-11-08T03:29:00.000Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"2018/11/08/dht-kademlia/","link":"","permalink":"https://velih.de/2018/11/08/dht-kademlia/","excerpt":"本文是论文 Kademlia: A Peer-to-Peer Information System Based on the XOR Metric 的翻译。","text":"本文是论文 Kademlia: A Peer-to-Peer Information System Based on the XOR Metric 的翻译。 Kademlia：一种基于 XOR 距离的 P2P 信息系统作者：Petar Maymounkov 以及 David Mazières Abstract（摘要）我们描述了一种在容易出错的环境下仍具有可证明的容错性和性能的 P2P 分布式散列表。我们的系统使用了一种基于 XOR 距离的新型拓扑来路由查询与定位节点的需求，该拓扑简化了算法及算法的证明。该拓扑有这样一个特点：每次信息交换都传递或者加强了（节点间）有效联系。系统根据这种联系来发送并行异步查询消息，从而做到容忍节点故障同时不对用户造成超时。 Introduction（介绍）本论文描述 Kademlia —— 一个 P2P 分布式散列表（DHT）。Kademlia 有许多之前的 DHT 无法同时提供的理性功能。它最大限度地减少了节点必须发送的用于互相了解的配置消息数量。查找 Key 时附带自动传播配置信息。节点具有足够的认知与灵活性来通过低延迟路径路由查询。Kademlia 使用并行异步查询来避免来自失败节点的超时延迟。节点记录其他节点的存在以阻挡某些基本的拒绝服务攻击。最后，只要对运行时间进行简单的假设就可以对 Kademlia 的几个重要特性进行正式证明。 Kademlia 采用了许多 DHT 使用的基本方法。密钥使用不透明的 160 位空间，比如 SHA-1 散列。参与的计算机在 160 位密钥空间中都有一个节点 ID。key-value 键值对存储在那些 ID “接近” key 的节点上。最终，一个基于节点 ID 的路由算法使得任何节点可以通过给出的目标 key 有效地定位 key 附近的节点。 Kademlia 的众多优点得益于它创新地使用 XOR 来度量节点间的距离。XOR 是对称的，允许 Kademlia 接收来自那些拥有完全相同的路由表信息的节点的查询请求。像是 Chord 这样没有这种特性的系统就无法从查询请求中获取有用的路由信息。甚者，不对称会导致严格的路由表。在 Chord 中，每个节点必须存储？？？？？？？ 为了定位接近特定 ID 的节点，Kademlia 自始至终仅使用一个路由算法。相反，其他系统使用一种算法来接近目标 ID，使用另一种算法来完成最后几跳。在现有的系统中，Kademlia 最像 Pastry 的第一阶段，虽然作者没有用这种方式描述，但是通过 Kademlia 的 XOR 距离发现目标 ID 的节点数可以减少一半。在 Pastry 的第二阶段，它将距离度量切换为 ID 间的数字差异。它还在复制中使用第二个数字差异度量。遗憾的是，第二个度量标准中相距较近的节点在第一个度量标准中则可能相差很远，以致在特定节点 ID 值处产生了不连续性，从而降低了性能，并使得对最坏情况行为的正式分析变得复杂。 System Description（系统描述）与其他 DHT 相同类似，Kademlia 给节点分配一个 160 位的透明 ID，并提供一个查询算法用以定位更加接近指定节点的节点，以对数级别收敛查询到目标节点。 Kademlia 高效地将节点作为一个二叉树的叶子节点，每个节点的位置取决于其 ID 的最短唯一前缀。图一展示了唯一前缀为 0011 的节点在树中的位置。对于任一给定的节点，我们将二叉树分为一系列连续的不包含该节点的子树。最高的子树由不包含该节点的整个二叉树的一半组成。接下来的子树由不包含该节点的剩余二叉树的一般组成。在图示中，节点 0011 的所有子树被圈出，按照高度由高到低排序，前缀分别是 0, 01, 000 以及 0010。 Kademlia 协议确保每个节点都知晓其每个子树中的至少一个节点。有了这个保证，任一节点都可以通过 ID 定位到另外的节点。图二展示了节点 0011 通过不断在子树中查询最佳它所知晓的最佳节点的方式最终找到目标节点 1110 的过程。 本节的剩余部分补充了查找算法的具体细节。我们首先定义一个精确的概念来描述节点间的距离，让我们指出距离某个 key 最近的 k 个节点提供了可能。然后给出一个查询协议，即使没有节点与给定的 key 拥有相同的唯一前缀或者给定节点的一些子树是空的，这个协议仍然是可工作的。 XOR Metric（XOR 度量）每个 Kademlia 节点都有一个 160 比特的节点 ID。节点 ID 目前只是随机的 160 位标识符，尽管它们同样可以像在 Chord 中那样构造。节点发送的每条消息都包含其节点ID，允许接收方在必要时记录发送方的存在。 键同样是 160 位标识符。为了给特定的节点分配 key-value 键值对，Kademlia 依赖两个标识符之间的距离的度量。给出两个 160 位的标识符 $x$ 和 $y$，Kademlia 将 $x$ 与 $y$ 的距离定义为 $x$ 与 $y$ 按位异或后的整数值，即 $d(x, y) &#x3D; x \\oplus y$。 我们首先注意到 XOR 运算是有效的，尽管属于非欧几里德距离。异或运算有以下性质： $$d(x, x) &#x3D; 0$$ $$d(x, y)&gt;0, \\text{ if } x \\neq y$$ $$\\forall x, y:d(x, y)&#x3D;d(y, x)$$ $$d(x,y)+d(y,z) \\geq d(x,z)$$ $$d(x,y) \\oplus d(y,z) &#x3D; d(x,z)$$ $$\\forall a \\geq 0, b \\geq 0: a+b \\geq a \\oplus b$$ 接下来我们注意到异或度量刻画了基于二叉树的系统草图中隐含的距离概念。在 160 位 ID 完全填充的二叉树中，两个 ID 间距离大小是包含它们的最小子树的高度。当树未完全填满时，与 ID $x$ 最接近的节点是节点 ID 与 $x$ 共享最长公共前缀的节点。如果树中有空分支，则可能有多个叶子节点具有最长的公共前缀，在这种情况下，与 $x$ 最接近的叶子将变为离通过翻转 $x$ 中对应于树的空分支中的位得到的 $\\widetilde{x}$ 最近的叶子。 类似于 Chord 算法的顺时针圆周度量，XOR 也是单向的。对于任意给定的点 $x$ 以及 $∆ &gt; 0$，仅存在一个点 $y$ 使得 $d(x, y) &#x3D; ∆$。单向性保证了对于相同的 key，无论从哪个原点开始查询，都会沿着相同的路径进行收敛。这样，沿着查询路径缓存 key-value 键值对就能减轻存放热门 key 值节点的压力。类似于 Pastry 而不同于 Chord， XOR 拓扑是对称的。 Node State（节点状态）Kademlia 节点存储了其他节点联系人的信息以路由查询消息。对于每个 $0 \\leq i &lt; 160$，每个节点构建一个（IP 地址，UDP 端口，节点 ID）的三元组列表用来存储与该节点相距 $[2^i$, $2^{i+1})$ 的节点。我们称之为 $K$ 桶。每个 $K$ 桶内节点根据节点最后可见时间进行排序，最早可见的放置在开头，最近可见的放置在末尾。对于较小的 $i$ 值，$K$ 桶通常是空的（因为不存在合适的节点）。对于较大的 $i$ 值，列表最大容量为 $k$，$k$ 是一个系统范围内的复制参数。仅当任意给定的 $k$ 个节点不会在一个小时内同时失效时，$k$ 值才有效，例如 $k &#x3D; 20$。 当一个 Kademlia 节点收到任一来自其他节点的请求或者回复消息，它会根据发送者的节点 ID 更新 $K$ 桶。规则如下： 如果发送节点已经存在接收方的 $K$ 桶中： 接收方将发送节点移动到相应 $K$ 桶的尾部； 如果发送节点并未存在接收方的 $K$ 桶中： 如果 $K$ 桶中节点数量少于 $k$ 个，则直接将发送节点插入至 $K$ 桶尾部； 如果 $K$ 桶已满： 如果最早可见的节点不能 ping 通，则移除该节点，然后将发送节点插入至 $K$ 桶尾部； 如果最早可见的节点可以 ping 通，则将该节点移动到尾部，然后丢弃发送节点； $K$ 桶高效地实现了 LRU 算法，只是活动的节点永远不会从列表中移除。这种对与旧联系人的偏好来自于我们的对于 Saroiu 等人从 Gnutella 网络收集的追踪数据的分析。图 3 展示了 Gnutella 节点随着当前时间的推移，额外在线一个小时的可能性。一个节点在线时间越长，它就越可能继续额外在线一个小时。通过保持最老的在线节点，$K$ 桶最大化了列表中节点在线的可能性。 $K$ 桶采用这样的更新策略的另一个优点在于可以抵挡特定的 DoS 攻击。攻击者不能使用新的节点泛洪系统来刷新节点的路由状态。只有当旧的节点离开了系统，Kademlia 才会插入新的节点。 Kademlia protocol（Kademlia 协议）Kademlia 协议由四个 RPC 调用组成：PING, STORE, FIND_NODE, FIND_VALUE。 PING 用于探测一个节点是否在线。 STORE 用于通知一个节点存储一个 key-value 对以便之后获取。 FIND_NODE 使用一个 160 比特的 ID 作为参数。调用对象返回一个其知晓的距离目标 ID 最近的 $k$ 个节点信息 &lt;IP 地址，UDP 端口，节点 ID&gt; 三元组。这 $k$ 个三元组可能来自单个 $K$ 桶，也可能因为距离最近的 $K$ 桶未满而取自多个 $K$ 桶。在任何一种情况下，RPC 调用接收者必须返回 $k$ 个节点，除非该节点知晓节点总数小于 $k$ 个。 FIND_VALUE 调用和 FIND_NODE 一样，不过当调用的接收者存有请求者所请求的键的时候，它将返回相应键的值。 在所有的 RPC 中，接收者必须返回一个 160 比特的随机 RPC ID，以防止地址伪造（？）。可以将 PING 消息附加在 RPC 消息的回复中以确定发送者的网络地址。 临近 ID 节点查找Kademlia 节点需要实现的最重要的过程就是根据一个节点 ID 找到与它最近的 $k$ 个节点。我们称这个过程为 节点查找，Kademlia 使用递归完成这个工作。由查找需求的起始节点先从离目标节点 ID 最近的非空 $K$ 桶中取出 $α$ 个节点（如果 $K$ 桶中节点不足 $α$ 个，则从其他 $K$ 桶中取满 $α$ 个）。起始接地并行向这 $α$ 个节点发送 FIND_NODE RPC 调用。$α$ 是一个系统级参数，例如 3。 在递归调用阶段，初始节点根据 $α$ 个节点返回的节点列表再次发送 FIND_NODE 请求（递归调用在所有 $α$ 个节点完成返回前就可以开始）。起始节点从其他节点返回的大小为 $k$ 的节点列表中选出 $α$ 个之前没有查询过的节点，向它们发送 FIND_NODE RPC 调用。失败的节点将不再会纳入考虑范围。如果有一轮 FIND_NODE 调用中返回的所有节点都不比之前知晓的 $k$ 个节点更加接近，则当初始节点完成对最近的这个 $k$ 个节点的查询并得到响应之后，查询结束。当 $a&#x3D;1$，查询算法在信息开销和故障节点的检查延迟上与 Chord 类似。但是 Kademlia 可以灵活地从 $k$ 个节点中选择任一一个发送请求以降低查询延迟。 存储键值对与重新发布时间上述查询过程实现了大部分的操作。为了存储一个 key-value 对，起始节点通过上述操作定位到 $k$ 个离 key 最近的节点，并向它们发送 STORE 调用。另外，每个节点会在需要的情况下重新发布 key-value 以使 key-value 持续存在。在 Kademlia 当前的应用中（文件分享），key-value 的原始发布者需要每个 24 小时重新进行发送。否则，key-value 将在发布 24 小时后失效，以限制系统中不可用的索引信息。对于其他的应用，比如数字证书或者用于值映射的加密哈希，更长的过期时间也许会更合适。 查询键值对与过期时间为了找到一个 key-value 对，节点发起一个节点查找以找到 ID 与 key 最接近的 $k$ 个节点。值查询使用的是 FIND_VALUE 调用而不是 FIND_NODE 调用。而且，如果任何一个节点返回了值，则这个过程立即结束。出于缓存的目的，一旦查找成功，发起节点将会将 key-value 存储至其知晓的相对 key 最近的，但是在查找过程中并没有返回值的节点。 由于拓扑是单向的，随后对于相同 key 的查找很可能在查询到最近的节点之前就命中存储在非最近节点中缓存。在某个 key 活跃度非常高的情况下，系统可能会将其存储在很多节点。为了避免这种过度存储，我们为 key-value 设置了过期时间，这个时间与节点 ID 相对于 key 的距离成指数反比。虽然简单的 LRU 策略会产生类似的生命周期分布，但是没有合适的方法去选择cache的大小，因为节点对于系统将要储存多少值没有先验知识。 $K$ 桶更新与加入网络$K$ 桶会根据经过节点的请求而保持更新。为了解决因为某个特殊的 ID 段长时间没有查询的特殊情况，节点会刷新那些在过去一小时内没有过查询请求的 $K$ 桶。刷新就是随机选取一个处在当前 $K$ 桶范围的 ID，然后对其进行节点查询。 为了加入网络，一个节点 $u$ 必须要与一个已存在在网络中的节点 w 有联系。$u$ 将 w 插入到合适的 $K$ 桶中，随后 $u$ 对自己的 nodeID 进行一次节点查询，最后，$u$ 会联系到离它邻居更远的节点，然后更新相应的 $K$ 桶。在刷新过程中，$u$ 填充了自己的 $K$ 桶同时也将自己插入到其他节点的 $K$ 桶中。 Routing Table（路由表）Kademlia 的基础路由表结构非常直观，但是需要对高度不平衡的树进行一些细节处理。路由表是一棵叶子节点为 $K$ 桶的二叉树。每个 $K$ 桶包含了一些具有公共前缀的 ID。$K$ 桶公共前缀就是其在二叉树中所处的位置。可见，每个 $K$ 桶覆盖了一部分 ID 空间，所有的 $K$ 桶集合无重叠地覆盖了整个 160 比特 ID 空间。 路由二叉树中的节点是按需动态分配的。图 4 表示了这一个过程。初始状态下，节点 $u$ 的路由树只有一个节点，一个 $K$ 桶覆盖了整个 ID 空间。当 $u$ 得到了一个新的联系人时，它将根据与新的联系人的距离，尝试将其插入到合适的 $K$ 桶中。插入的规则如下： 如果 $K$ 桶未满，新的联系人就会被直接插入； 如果 $K$ 桶满了： 如果 $K$ 桶的范围包括了 $u$ 自身的 ID，则 $K$ 桶将会一分为二，原 $K$ 桶中的 ID 重新分配到新的 $K$ 桶中，然后再尝试插入； 如果 $K$ 桶的范围不包括 $u$ 自身的 ID，新的联系人将直接被丢弃；（3） 在这样高度不平衡的二叉树中会出现一个问题，导致这种情况下并不会按照规则 3 进行插入。假设一个系统中，$u$ 是唯一一个 ID 以 000 开头的节点，同时有超过 $k$ 个 ID 以 001 开头的节点 $v_1,v_2…v_k…v_n$。对于每个节点 $v$ 的路由树，$u$ 都将会被插入到一个空 $K$ 桶中。但是由于按照规则 3， $u$ 的桶更新只会通知到 $u$ 的路由树所记录的所有 $v$ 中的 $k$ 个。为了避免剩余的 $v$ 无法得到 $u$ 的桶更新信息，Kademlia 节点通过拆分以保存了超过 $k$ 个的所有有效的节点，即使不是因为 $K$ 桶中包括了自身的 ID。图 5 展示了这种额外的拆分。当 $u$ 刷新了桶，所有前缀为 001 的节点都会知晓。 Efficient Key Re-publishing（Key 的高效重发布）为了保证 key-value 对的持久存在，节点必须周期性重发布 key。否则，两种情况下可能导致对有效 key 的查询失败。 一些获得 key-value 对的节点离开了网络 新的相比原始被发布过 key-value 对的节点 ID 更近的节点加入了网络 在这两种情况下，拥有这个 key-value 对的节点需要将其重新发布到离 key 最近的 $k$ 个节点。 为了弥补节点离开网络的情况，节点每隔一个小时重新发布 key-value 对，一个直接的实现就是将会产生很多消息——每个存储这个 key-value 对的节点（至多 $k$ 个）间隔一小时会发起一次查询，然后向其他 $k-1$ 个节点发起 STORE 请求。这个重新发布操作可以大大简化。首先，当一个节点收到对某个 key-value 对的 STORE 调用，它假设这个 RPC 调用同样被发送给了另外 $k-1$ 个节点，这个节点在下一个小时不会再重新发布此 key-value 对。这保证了知道复制间隔不是完全同步的，每隔小时内只有一个节点会重新发布 key-value 对。 第二个优化在于在重新发布 key 之前避免进行节点查询。像上一节所说的，为了处理不平衡树，在必要的时候节点将会拆分 $K$ 桶来确保它对至少 $k$ 个节点的周围子树充分了解。如果节点在重新发布 key-value 之前更新了该 $k$ 节点子树的所有 $K$ 桶，它将自动获知距离给定 key 最近的 $k$ 个节点。对于这些 $K$ 桶的更新代价可以分摊给许多 key 的重新发布（平均而言，这样就降低了单个 key 重新发布时的代价）。 要搞清楚为什么在 $u$ 更新规模大于等于 $k$ 的子树的所有 $K$ 桶后不再需要节点查询了，需要分为两种情况。如果要被充发布的 key 在该子树的 ID 范围内，那么因为子树的规模至少为 $k$ 而且 $u$ 具有该子树的全部知识，显然 $u$ 肯定知道距离 key 的最近 $k$ 个节点。如果要被充发布的 key 在该子树的 ID 范围外，因为 $u$ 是 $k$ 个距离 key 最近的节点之一（否则 $u$ 不会存储关于该 key 的信息），显然所有距离该 key 比距离子树更近一些的 $K$ 桶中的元素都少于 $k$。因此，$u$ 将会知晓所有这些 $K$ 桶中的所有节点，再加上关于子树的知识，就可以得到距离该 key 最近的 $k$ 个节点。 当一个新节点加入网络，对于每个 key-vaule 对来说，如果该节点为其 $k$ 个最近节点之一，那么必须对其进行存储。网路中原有的节点同样可以通过其边缘子树的完整知识，知道哪些 key-value 对需要存储在该新增节点上。每个了解到新节点的节点都会发起 STORE 调用把相关的 key-value 对传送到新节点之上。为了避免重复的 STORE 调用 ，只有那些自身 ID 比其他节点 ID 更接近 key 的节点才会进行 key-value 对的传送。 Sketch of Proof（待译）待翻译 Implementation Notes（待译）待翻译 Summary（总结）由于采用基于 XOR 度量的拓扑，Kademlia 是第一个结合了可证明的一致性，性能，低时延路由以及单向对称拓扑的点对点系统。Kademlia 还引入了并发参数 $α$，它允许人们在带宽中交换常数因子，以实现异步最低延迟路由选择和无延迟错误恢复。最后，Kademlia 是第一个利用节点故障与正常运行时间成反比关系这一事实的点对点系统。 参考 Kademlia ：一种基于 XOR 度量的 P2P 信息系统 P2P网络–Kademlia协议","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://velih.de/tags/Algorithm/"},{"name":"翻译","slug":"翻译","permalink":"https://velih.de/tags/%E7%BF%BB%E8%AF%91/"}]},{"title":"在 IPFS 上部署静态博客","slug":"ipfs-blog","date":"2018-09-22T13:12:00.000Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"2018/09/22/ipfs-blog/","link":"","permalink":"https://velih.de/2018/09/22/ipfs-blog/","excerpt":"本文主要记录在 IPFS 上部署博客的过程，用以熟悉 IPFS 的基本操作。https://ipfs.0ranga.com 就是博主博客在 IPFS 上部署的版本。","text":"本文主要记录在 IPFS 上部署博客的过程，用以熟悉 IPFS 的基本操作。https://ipfs.0ranga.com 就是博主博客在 IPFS 上部署的版本。 IPFS安装 IPFS首先得在计算机上安装 IPFS，博主 PC 的操作系统的 Arch，可以直接使用包管理器进行安装。如果之后需要为博客添加域名则需要在云服务器上部署 IPFS，博主选择的是 DightOcean 的 CentOS 7。所以提供了以上两种操作系统的安装方式，其他系统如何安装请自行探索。 12345678910111213# Arch## 包管理器直接安装pacman -S go-ipfs # CentOS## 从 https://dist.ipfs.io/#go-ipfs 获取最新安装包，例如wget https://dist.ipfs.io/go-ipfs/v0.4.17/go-ipfs_v0.4.17_linux-amd64.tar.gz## 解压tar xvfz go-ipfs_v0.4.17_linux-amd64.tar.gz## 安装cd go-ipfs &amp;&amp; ./install.sh 安装完成后需要先初始化 IPFS 1ipfs init 另外，一般在云服务器上需要将 IPFS 设置为自启动服务，设置方式如下，首先需要添加 service 描述文件 1vi /etc/systemd/system/ipfs.service 文件内容如下： 123456789[Unit]Description=IPFS Daemon[Service]Type=simpleExecStart=/usr/local/bin/ipfs daemon --enable-namesys-pubsub[Install]WantedBy=multi-user.target 然后使服务开机自启动同时立即启动： 12systemctl enable ipfssystemctl start ipfs 部署一个文件接下来在本地创建一个文件 index.html，在里面写入简单的话 1echo &#x27;hello, my simple blog!&#x27; &gt; index.html 随后添加至 IPFS 中 1ipfs add index.html 得到返回如下 123ipfs add index.html # added QmaqwGFUj34wDLPzHVxmJhEN6n27xidsfrmf2WUnhSTKTr index.html# 23 B / 23 B [=========================================] 100.00% 可以通过如下命令查看文件内容 1ipfs cat QmaqwGFUj34wDLPzHVxmJhEN6n27xidsfrmf2WUnhSTKTr 当我们使用如下命令启动节点后，文件内容将会逐渐被 IPFS 网络存储 123ipfs daemon# 或者systemctl start ipfs 随后可以通过访问 https://gateway.ipfs.io/ipfs/QmaqwGFUj34wDLPzHVxmJhEN6n27xidsfrmf2WUnhSTKTr 得到该文件。如果你不介意将这句简单的话作为你的博客的全部内容的话，那至此博客的全部创建完成了，而且理论上永远不会消失。 部署静态博客框架：以 Hexo 为例虽然小标题写的是“以 Hexo 为例”，但本文不会细讲如何使用 Hexo，Hexo 官方文档提供了细致的帮助。如果你之前用过 Hexo 而且因为懒得写作而把它丢弃在硬盘里，这里有几个命令可以帮助你快速回想起使用方法。 12345678910# 创建新文章hexo new post &lt;new_post_name&gt;# 清理hexo clean# 生成博客hexo g# 启动本地服务器hexo s --open# 部署hexo d 在使用 hexo g 生成静态博客后，我们可以在 public 目录中找到部署博客需要的所有文件，入口是熟悉的 index.html。（其他博客框架也有个类似 public 的文件夹用于存放整个博客） 使用如下命令将整个 public 文件夹放入 IPFS 1ipfs add -r public 添加后的输出中最后一行是目录 public 的哈希值，比如博主得到的是 1# added QmSc5D1ahPbVkAhHFxJvqnWDEWrgMQw9B9BGmQv5i1VAwt public 然后访问 https://gateway.ipfs.io/ipfs/QmSc5D1ahPbVkAhHFxJvqnWDEWrgMQw9B9BGmQv5i1VAwt/ 就可以看到保存在 IPFS 上的博客了。不过，在修改文章更新 public 目录之后，目录的哈希值会变化，因此以上的连接只是博客在当前的一个（永久）状态，如果想要通过一个链接访问不断更新的博客，就需要借助 IPNS，IPNS 的作用就是将某个文件与 PeerID 进行绑定，一般情况下，PeerID 可以保持不变，这样，通过访问 PeerID，就可以访问与 PeerID 绑定的内容了，这和 DNS 中域名与 IP 的关系类似。 通过以下命令将当前的 public 目录哈希与 PeerID，PeerID 不需要显示指定，得到绑定结果后的 PeerID 实际上与通过 ipfs id 命令得到的 ID 一致。 123456789101112131415# 绑定ipfs name publish &lt;file_hash&gt;ipfs name publish QmSc5D1ahPbVkAhHFxJvqnWDEWrgMQw9B9BGmQv5i1VAwt# 绑定后的输出# Published to QmRP5ZT2B5W8zWTXhPwpgZQpuj8Gv5bNaXWbttRB6niAYo:# /ipfs/QmSc5D1ahPbVkAhHFxJvqnWDEWrgMQw9B9BGmQv5i1VAwt# 访问（列出目录） IPNS QmRP5ZT2B5W8zWTXhPwpgZQpuj8Gv5bNaXWbttRB6niAYoipfs ls /ipns/&lt;peer_id&gt;ipfs ls /ipns/QmRP5ZT2B5W8zWTXhPwpgZQpuj8Gv5bNaXWbttRB6niAYo# 反向解析ipfs name resolve &lt;peer_id&gt;ipfs name resolve QmRP5ZT2B5W8zWTXhPwpgZQpuj8Gv5bNaXWbttRB6niAYo 绑定 IPNS 之后，可以通过 https://gateway.ipfs.io/ipns/QmRP5ZT2B5W8zWTXhPwpgZQpuj8Gv5bNaXWbttRB6niAYo/ 访问到博客。 但是直接部署 public 目录存在一个问题：hexo 这类的博客中所有本地资源的路径均为根目录开头的相对站点根目录的绝对路径，例如对于 /css/home.css，当访问 https://gateway.ipfs.io/ipfs/&lt;file_hash&gt; 这样在文件中有类似绝对路径链接时，相当于访问 https://gateway.ipfs.io/css/home.css，显然这个 home.css 文件是不存在的。如果想要使得网站可以正确访问，有两种解决办法： 使用相对路径 ./css/home.css，这样就相当于访问 https://gateway.ipfs.io/ipfs/&lt;file_hash&gt;/css/home.css，如此可以正确访问。如果博客框架自身支持将所有本站资源的绝对路径替换成这样，或者整个博客是自己实现如此，那不需要额外的操作就可以得到一个完美的博客。 让 http://gateway.site/css/home.css 真实存在，即将 http://gateway.ipfs.io/ipfs/&lt;file_hash&gt; 替换成一个独立域名 http://gateway.site，这就需要用到 IPFS 提供的 dnslink 增强。 为博客添加域名还是以我自己的域名 0ranga.com 为例，这个域名托管在 Namecheap 上，使用 Namecheap 的 DNS 管理，可以为域名增加一个子域名 TXT 记录如下： 1TXT 0ranga.com dnslink=/ipns/QmRP5ZT2B5W8zWTXhPwpgZQpuj8Gv5bNaXWbttRB6niAYo 这样，我们可以通过地址 https://gateway.ipfs.io/ipns/0ranga.com 访问到部署在 IPFS 上博客。我们还可以进一步缩短访问链接长度，在 DNS 管理中进一步添加如下记录（为了不与原博客 0ranga.com 冲突，这里使用子域名 ipfs.0ranga.com）： 123456# 1. 为子域名 ipfs.0ranga.com 添加 A 记录，绑定到一个 Gateway 的 IP，# 示例中是 209.94.78.78 ，这个 IP 是 gateway.ipfs.io 的众多 IP 中的一个A ipfs.0ranga.com 209.94.78.78# 2. 为子域名 ipfs.0ranga.com 添加 TXT 记录，指定 IPNSTXT ipfs.0ranga.com dnslink=/ipns/QmRP5ZT2B5W8zWTXhPwpgZQpuj8Gv5bNaXWbttRB6niAYo 等待 DNS 记录生效后，可以通过 http://ipfs.0ranga.com 访问博客。至此，博客的短链接改造已经基本完成。接下来会记录些自己另外的一些尝试，包括对博客的访问进行加速等。 进阶部署云服务器以增强博客的可用性同时加快访问速度如果部署了博客的本地 PC 关机，同时博客的访问量不是那么可观，则可能存在一部分博客文件无法访问的情况，因为在没有激励的情况下 IPFS 上的其他节点一般不会主动存储其他用户往 IPFS 中添加的文件。所以配置一个云服务器来存储博客本身就可以确保文件的可用性（请问：那为什么还要用 IPFS 来部署博客？），同时随着 FileCoin 激励与 IPFS 的结合，事情会变得不那么糟糕。 云服务部署的步骤如下： 按照安装 IPFS 这小节中的安装方法在云服务器上部署一个 IPFS 服务； 为了 Gateway 可以被公网访问，需要将 IPFS 配置文件 ～/.ipfs/config 中 Addresses:Gateway 的配置改为 /ip4/0.0.0.0/tcp/80 ，用以监听所有网卡上的 TCP 80 端口，随后重启 IPFS 服务使配置生效； 通过本地浏览器直接访问云服务器的 IP （博主目前测试使用的服务器实例的 IP 为 104.248.70.88）结果为 404 page not found ，则表明服务部署成功。进一步，访问 http://104.248.70.88/ipfs/QmaqwGFUj34wDLPzHVxmJhEN6n27xidsfrmf2WUnhSTKTr 可以得到 部署一个文件 这节中的 index.html 文件，至此完成了 IPFS Gateway 的部署与共享； 为 Gateway 绑定域名。类似之前的做法，添加一条 DNS A 记录。待记录生效后，访问连接变为 http://gateway.0ranga.com/ipfs/QmaqwGFUj34wDLPzHVxmJhEN6n27xidsfrmf2WUnhSTKTr 1A gateway.0ranga.com 104.248.70.88 改用自己的 Gateway。将之前配置的子域名 ipfs.0ranga.com 的 A 记录对应的 IP 改成云服务器的 IP（TXT 记录保持不变），当浏览器访问 ipfs.0ranga.com 这个域名时，首先会访问 104.248.70.88 这台 Gateway 服务器，Gateway 再向整个 IPFS 网络收集当前页面的碎片； 1A ipfs.0ranga.com 104.248.70.88 通过 Pin 操作将本地博客 public 目录”钉“至云服务器 IPFS 节点，然后将 public 目录哈希绑定至云服务器中 IPFS 的 PeerID，另外记得修改子域名 ipfs.0ranga.com 的 TXT 记录中的 IPNS 地址。此后即使关闭本地 PC 的 IPFS 服务，部署在 IPFS 上博客也可以被正常访问，访问地址依然是 http://ipfs.0ranga.com； 1ipfs pin add QmSc5D1ahPbVkAhHFxJvqnWDEWrgMQw9B9BGmQv5i1VAwt 如果觉着同步文件至云服务器，而又不介意一直运行本地 PC 的 IPFS 服务，但是博客的访问速度又不快，可以考虑使用以下方式将云服务器上 IPFS 节点直接加入到本地 Peer 列表中，这样可以帮助云服务器上的 IPFS 节点快速找到本地 PC IPFS 节点中的博客文件。 1ipfs swarm connect /ip4/104.248.70.88/tcp/4001/ipfs/QmamxGp6sw2dKUHm2RnaJ7zRDeR3w8m98kKAYgfpszeqHR HTTPS 加持在云服务器 CentOS 系统上安装 Nginx 服务，然后使用 Certbot 生成 Let’s Encrypt 证书，过程比较简单，Certbot 的安装在官方教程有详细的说明，此处省略。最终，子域名 ipfs.0ranga.com 的 Nginx 的配置大致如下： 123456789101112131415161718192021222324252627server &#123; server_name ipfs.0ranga.com; location / &#123; proxy_pass http://localhost:8080/; proxy_set_header Host $host; proxy_buffering off; proxy_pass_request_headers on; &#125; listen 443 ssl; # managed by Certbot ssl_certificate /etc/letsencrypt/live/ipfs.0ranga.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/ipfs.0ranga.com/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot&#125;server &#123; if ($host = ipfs.0ranga.com) &#123; return 301 https://$host$request_uri; &#125; # managed by Certbot listen 80 default_server; server_name ipfs.0ranga.com;return 404; # managed by Certbot&#125; 注意，由于端口由 Nginx 进行控制， Gateway 不能再占用 80 端口，因此这里将其（～/.ipfs/config 中 Addresses:Gateway 配置）重新设定为 8080。gateway.0ranga.com 的 Nginx 配置也是类似。Hurray！现在我们可以通过 https://ipfs.0ranga.com 访问博主部署在 IPFS 上的博客，通过 https://gateway.0ranga.com 访问博主的 Gateway 了。 总结可以用以下几个网址进行总结： 序号 链接 改进之处 1 https://gateway.ipfs.io/ipfs/&#x2F; 原始博客目录 2 https://gateway.ipfs.io/ipns/&#x2F; IPNS 映射目录，站点更新时链接保持不变 3 https://gateway.ipfs.io/ipns/0ranga.com dnslink 绑定，增加链接可读性 4 http://ipfs.0ranga.com 实现完全自定义域名 5 https://ipfs.0ranga.com 为域名添加 HTTPS 证书 参考资料 https://michalzalecki.com/set-up-ipfs-node-on-the-server/ https://github.com/ipfs/notes/issues/39 https://www.cloudxns.net/Support/detail/id/304.html https://ipfs.io/ipns/Qme48wyZ7LaF9gC5693DZyJBtehgaFhaKycESroemD5fNX/post/putting_this_blog_on_ipfs/","categories":[],"tags":[{"name":"IPFS","slug":"IPFS","permalink":"https://velih.de/tags/IPFS/"}]},{"title":"BT 增强建议之 Peer","slug":"bt-peer","date":"2018-08-30T11:14:00.000Z","updated":"2020-07-20T13:24:35.000Z","comments":true,"path":"2018/08/30/bt-peer/","link":"","permalink":"https://velih.de/2018/08/30/bt-peer/","excerpt":"","text":"本文是 BT 系列文章中的一篇，主要介绍 Peer 以及 Peer 间的通信，有需要的话可以先阅读博文 BT 增强建议之概述。 Peer 来源在讲 Peer 间的通信前，先总结一下 Peer 的来源。 Magnet：磁力连接中有 x.pe 参数可以预设一些 Peer； Tracker：Tracker 服务器的作用就是提供 Peer； Local Service Discovery（BEP14）:通过对本地组播地址 239.192.152.143:6771 和 [ff15::efc0:988f]:6771 发出 info_hash 宣告来尝试获得响应，如果有响应，则添加为 Peer； Peer Exchange：通过 Peer 间的 Peer Exchange 扩展消息来与其他 Peer 交换 Peer，后面会详细提到； DHT：通过 DHT 网络获取； 协议概述Peer 间的通信属于应用层协议，它使用的应用层协议可以是 TCP 或者 µTP。 Peer 间按照元数据中描述的文件片段索引（起始索引为 0）进行数据交换，当一个 peer 下载完成一个分片而且经过了通过了 hash 值的校验，它会通知它的所有 peer 自己拥有了这个片段。 Peer 连接的 Peer 会有一定上限数量，所有的 Peer 按照 BEP40 - Canonical Peer Priority 中定义的基于双方 IP 的 crc32-c 值的算法进行排序，选择这个值更小的若干 Peer 进行连接。 每个连接会包含两个状态位，choked or not（表示是否对对方 choked），interested or not（表示是否对对方的数据感兴趣），状态在连接建立时的初始值为 choked 以及 not interested。因为交互是双方的，一方不仅要存储自己给对方设定的状态，也要存储对方给自己设定的状态，因此实际上会有四个状态位（BEP 中表示只有两个状态位，但是由于主动与被动语态的复杂性以及考虑到实际源码的实现，这里引入成四个状态位，增加了一定的冗余，但便于写作，一定程度上也便于理解）。 假设 A，B 是在下载同一个资源的对等体。用 A.B 表示 peer A 中 A 与 B 的连接。所以 A 与 B 连接建立时，在 A 的内存空间中的保存的对于 B 的连接会有如下的状态： 1234choked = true;interested = false;peer_choked = true;peer_interested = false; 同样道理，B 中也会有类似的这样四个状态。且初始状态相同。只有当 A 的 interested == true 而且 peer_choked == false 时，A 才能向 B 索要数据。 对于 A 而言，有下面四个时机更新这一组状态： 在收到 B 发来的 choke&#x2F;unchoke 消息时，会更新 peer_choked 状态置为 true&#x2F;false； 在收到 B 发来的 interested，not interested 消息时，会更新 peer_interested 状态置为 true&#x2F;false； 不断根据自己所拥有的分片与 B 拥有分片的状态（B 通过 bitfield 消息告知 A）更新 interested 状态，并通过发送 interested&#x2F;not interested 消息给 B； 消息类型handshake 是连接 peer 间连接建立后发送的第一个消息，主要用于通告 info_hash 与 peer_id： 1| 19 (1) | &quot;BitTorrent protocol&quot; (19) | reserved_byte (8) | info_hash (20) | peer_id (20) | keepalive，第一个字段为消息体长度，keepalive 消息长度体为 0： 1| len (4) | 除基础消息外的其他消息均有固定的格式如下：消息体长度 + 类型 + 负载。 1| len (4) | type (1) | payload | 可能的类型值有： 0 - choke 1 - unchoke 2 - interested 3 - not interested 4 - have 5 - bitfield 6 - request 7 - piece 8 - cancel 20 - extend message 其中 choke，unchoke，interested，not interested 消息没有负载。 bitfield 消息仅在 handshake 后双方各发送一次，每个比特表示对应的分片是否完整。 1| len (4) | 5 (1) | bitfield | have 消息的负载是一个整型数字，peer 在一个分片下载完成并校验通过后发送该附带刚下载完成分片的索引的消息给其他 peer。 1| len (4) | 4 (1) | index (4) | request 消息结构如下，用于请求指定分片（index）的范围为 [ offset, offset + length ) 的字节，length 一般为 2^14 (16 kiB)，超过则会关闭连接。 1| len (4) | 6 (1) | index (4) | offset (4) | length (4) | piece 消息用于回复 request 消息，即返回自身分片（index）的范围为 [ offset, offset + length ) 的字节 1| len (4) | 7 (1) | index (4) | offset (4) | block (length) | cancel 消息除了类型和 request 消息不同外，其他均与 request 一致，用于向 peer 取消之前发出的 request 请求。这是由于为了加快最后若干分片的下载速度，客户端会启用 Endgame 模式，这个模式下，peer 会向所有的 peer 请求相同的分片片段，当 peer 从某个 peer 获得所需的分片片段后，需要向剩余的 peer 发送 cancel 消息以减少不必要的传输。 1| len (4) | 8 (1) | index (4) | offset (4) | length (4) | 类型为 20 的是扩展消息。 扩展消息BEP10 - Extension Protocol 中定义了扩展消息。peer 通过将 handshake 消息的保留字节的右数第 20 个比特置为 1 来通告其他 peer 自身支持扩展消息。即可以通过判断表达式 reserved_byte[5] &amp; 0x10 来判断 peer 是否支持扩展消息。 扩展消息的基础结构如下，实际上相当于是类型为 20 的普通消息。 1| len (4) | 20 (1) | extended_messag_id (1) | payload | extended_messag_id 为 0 的消息是 Extend Handshake 消息。 Extend HandshakeExtend Handshake 消息的有效载荷是一个 bencode 字典。字典中的所有键都是可选的，peer 需要忽略所有自己不支持的键。可选的键（还可以有更多）如下: m：字典，表示支持的扩展消息，键是扩展消息名称，值是扩展消息的 id（与 extended_messag_id 对应）。peer 通过这个键通告其他 peer 自己支持的消息类型，且该 peer 之后发送其他的扩展消息就会使用在这里对应的 extended_messag_id。扩展消息名称的格式一般为 客户端缩写_消息名称，这样可以实现全网全局消息类型唯一（侧重实现）；而 extended_messag_id 则是自己客户端自己定义，在 m 字典中不重复即可（侧重索引），这样便可以做到单向通信唯一。 p：整型，表示本地监听端口。帮助另一方了解自己的端口信息。连接的接收方是不需要发送这个扩展消息的，因为接收方的端口是已知的。（实际上 peer 间通信无论是基于 TCP 还是 µTP（UDP），接收方理论上都可以从传输层获得端口数据）。 v：字符串，表示客户端的名称与版本，这个比 peer id 更可靠一些。 yourip：字符串，表示在 peer 视角中对方 peer 的 IP，一般客户端通过此获取自己的公网 IP。 ipv6：字符串，表示自身压缩格式的 IPv6 地址。可能对方 peer 更喜欢使用 IPv6 地址。 ipv4：字符串，表示自身压缩格式的 IPv4 地址。可能对方 peer 更喜欢使用 IPv4 地址。 reqq：整型，表示自身的在不丢弃消息情况下可以保留的未处理的消息数量。在 libtorrent 中这个值是 250。 这个消息需要在普通 handshake 成功后立即发送，在连接的生命周期内这个 Extend Handshake 多次发送都是有效的，但实际实现中有可能被忽略。如果后续的 Extend Handshake 消息指定 m 字典中某些扩展的扩展 id 为 0，则表示禁用这些扩展。 Metadata Request(ut_metadata)BEP9 - Extension for Peers to Send Metadata Files 中定义了磁力链接，同时也定义了用于从 Peer 获取元数据的扩展消息 UT Metadata。 如果一个 peer 的客户端支持 UT Metadata 消息，那么在该 peer 向其他 peer 发送 Extend Handshake 消息时，需要在字典 m 中加入 ut_metadata 这个键，同时保持其对应的消息 id 在 m 字典中唯一。特殊的是对于这个消息的支持还需要在 Extend Handshake 消息负载字典中加入一个键 metadata_size，表示元数据的字节数。 UT Metadata 消息的负载也是一个 bencode 字典，有如下的键： msg_type：整型，代表消息类型，可能的类型有： request：0。请求类型，即请求序号为 piece 的 metadata 片段。请求类型的返回类型为 data 或者 reject； data：1。正常返回序号为 piece 的 metadata 片段。元数据会按照 16 kiB 大小切分，除了最后一片段，其余的都应该为 16 kiB 大小，序号也由此分割大小得来。元数据片段作为负载的一部分跟在整个字典后面，其并不使用 bencode 编码，但是长度需要计算在 len 中。 reject：2。表示被请求的 peer 没有序号为 piece 的 metadata 片段。也有可能是一定时间内请求超过数量限制，为了防止洪泛攻击，直接表示拒绝。 piece：指定元数据的分片序号。 total_size：仅在 data 类型消息中出现，和握手消息中的 metadata_size 语义一致。 Partial Seeds这个扩展是为了让 BT 支持对 Partial Seeds（部分种子，BEP21 - Extension for partial seeds）的识别与进一步优化。部分种子就是资源不完整但是也不再进行下载的 peer。这种情况发生在多文件种子中，用户只设定下载一部分文件。 这个扩展不定义额外的扩展消息，但是在扩展握手消息的字典中加入一个键 upload_only，值为整型，如果 peer 对下载不感兴趣则需要讲此值置为 1。 在 Tracker 的 Scrape 请求回复中，定义了 complete, incomplete 以及 downloaded 三种状态的 peer。为了让其他 peer 可以通过 Tracker 知晓 Partial Seeds 的情况，扩展定义了在 Scrape 回复中加入类型 downloaders，表示处于活跃状态，未完成下载且仍然需要继续下载的 peer 数量，Partial Seed 的数量可以通过 incomplete - downloaders 得到。同时让 Tracker 知晓 peer 自身处于 Partial Seed 状态，则需要通过 event=paused 事件进行告知，且每次通告时都要发送该事件。 Peer Exchange(ut_pex)Peer Exchange(PEX) 用于在 peer 间交换 peer 列表。通过在 Extend Handshake 消息的 字典 m 中加入 ut_pex 这个消息名称来表明支持，同样道理，消息 id 在 m 字典中保持唯一即可。 PEX 消息的负载也是一个 bencode 字典，有如下的键： added：当前连接的 IPv4 peer 压缩格式列表，告知对方进行添加 added.f：当前连接的 IPv4 peer 标志位，每个 peer 一个字节 added6：当前连接的 IPv6 peer 压缩格式列表，告知对方进行添加 added6.f：当前连接的 IPv6 peer 压缩格式列表标志位 dropped：过去断开连接的 IPv4 peer 压缩格式列表，告知对方进行删除 dropped6：过去断开连接的 IPv6 peer 压缩格式列表，告知对方进行删除 标志位定义如下： 0x02：属于 seed 或者 partial seed 0x04：支持 uTP 0x01：prefers encryption, as indicated by e field in extension handshake 0x08：peer indicated ut_holepunch support in extension handshake 0x10：outgoing connection, peer is reachable 发送规则 如果 peer 与某些 peer 断开连接，则需要需要在适当时候发送 PEX 消息，将断开连接的 peer 放在 dropped 中； 每分钟发送的 PEX 消息不能超过一条； 不需要在握手后立即发送 PEX 消息，在收集满一定的 peer 之后再发送效果更好； 添加或删除的 peer 列表中不能包括重复项，也不能在同一个 PEX 消息中删除添加的 peer； 除了最初的 PEX 消息之外，每条消息中添加的 peer 数量或者 删除的 peer 数量均不能超过 50 条； added, added6, dropped, dropped6 四个键中至少需要有一个； peer 可能会与严重违反这些规则的 peer 断开连接； 扩充 seed每个 peer 会执行如下的执行一些规则来断开与部分 peer 的连接。比如 在作种的时候会断开与 seed 和 partial seed 的连接； 根据 peer id 断开 IPv4 以及 IPv6 地址实际上属于同一个 peer 的一个连接，保留自己偏爱的地址家族连接； 这样的策略下，在 seed 占主导地位的 swarm 中通过 PEX 传播的活跃 peer 会不足。类似地，在 IPv4 占主导的 swarm 中，只支持 IPv6 的 peer 将很难获得 IPv6 的 peer。这很大程度降低了 PEX 消息的有效性。 为了解决这些问题，如果一个 peer 连接到一个特定地址家族的 peer 少于25个，活跃度的要求就会放宽。因为一下原因导致连接断开的 peer 也会被保存下来，并有资格被发在 PEX 消息的 added 中： 因为 peer id 相同而被断开的另一个地址家族的连接； 因缺乏兴趣而断开连接的 peer，比如对方是 seed&#x2F;partial seed 或者对方拥有的分片不满足自己的需求； 因为超过本地资源限制而断开的连接，比如全局的连接上限； 为了保证 peer 的有效性，因为这些原因添加到 added 中的 peer 只能通过 PEX 消息发送一次，发送完后必须从等待发送列表中删除。 安全性通过 PEX 获得的 peer 应该视为不可信的。攻击者可能通多伪造 PEX 消息来攻击这个 swarm。攻击者也可能通过 PEX 消息诱导 BT 客户端对特定 IP 进行尝试连接而引发 DDoS 攻击。 为了缓解这些问题，peer 应该避免从单个 PEX 源获取其所有连接作为候选连接。应忽略具有不同的端口的重复 IP，还可以根据 peer 的优先级来进行（协议概述中提到）排序。 快速扩展（Fast Extension）另外还有类似扩展消息的快速扩展消息，通过将握手消息的 reserved_byte[7] |= 0x04 来通告支持快速扩展，这里只列出快速扩展消息的种类，具体协议格式可参见 BEP06 - Fast Extension。 Have All&#x2F;Have None：来表示拥有所有分片或者未拥有任何分片，是 bitfield 消息的快速版本； Suggest Piece：建议其他 peer 下载某分片； Reject Request：拒绝 peer 对某个片段的请求； Allowed Fast：表示如果 peer 请求这个分片，即使它处于 choked 状态也会给它； lt Dont Have：在某些情况下（比如资源短缺，LRU Cache 过期）会导致 peer 不再拥有某个片段，则可以通过此消息告知其他 peer，该扩展定义在 BEP54 - The lt_donthave extension 中； 分片选择策略选择一个好的分片下载顺序与否对下载性能有这很大影响。如果选择了一个差的分片下载选择算法，则某一时刻可能所有分片你都可以下载，但是之后就没有你想下载的分片了。BT 中执行一下策略： Strict Priority（严格模式）一旦请求了某个分片的子片段，那么就会在请求其他子片段之前请求该特定分片的剩余子片段，以尽量优先获得这个完整的分片。 Rarest First（稀有优先）在选择接下来下载哪个分片时，peer 会选择最稀有的分片（自己没有这个分片，同时其他 peer 有，但是有这个分片的 peer 数量相对其他分片最少）进行下载。这个算法保证了不稀有的分片在之后仍然能被下载到，同时稀有的分片在逐渐变多。通过尽快复制最稀有的分片，减小了稀有分片在当前连接的 peer 中完全消失的可能性。 Random First Piece（随机首分片）当下载开始时，不会使用稀有优先算法。开始时 peer 没有分片可以用于上传，所以最重要的是尽快得到一个完整的分片。稀有的分片往往只被某一个 peer 拥有，从这个 peer 处下载这个分片（分成多个子片段）将会慢于从多个 peer 处下载相同分片的不同子片段。出于这个原因，刚开始下载时，会随机选择一个分片进行下载，随后策略转为稀有优先。 Endgame Mode有时从一个 peer 请求某个分片会很慢，这在下载整个资源你的中途不会是一个问题（因为中途同时发生不少请求），但是这种情况可能会影响最终的即将下载完成阶段。当所有剩余的子片段都已经在向其他 peer 请求时，它会同时向所有的 peer 请求这些子片段。当某一个 peer 返回了一个子片段，就向剩余的 peer 发送 cancel 消息以节约带宽。在实践过程中，Endgame 模式持续时间非常短，所以浪费的带宽不多，而且使得资源的最后一部分下载非常快。 Choking 算法BT 没有中心化的资源分配，每个 peer 有责任去最大化自己的下载速率。Peer 执行一种变种 tit-fot-tat 策略，从与自己相连的 peer 处下载分片，并选择合适的 peer 进行上传，对其他 peer 进行 choke。choke 表现为拒绝上传，但下载仍可继续，同时连接被保持不销毁，在 choke 结束后连接不需要重建。Choking 算法对于 BT 来说不是必须的，但是如果需要有一个好的下载性能是非常重要的。一个好的 choking 算法需要利用好所有的资源，提供好的上传给其他 peer，同时惩罚那些只下载不上传的 peer。 BT 中使用的变种 tit-fot-tat 策略是囚徒困境的应用，博主 youxu 的文章 P2P客户端的策略和奇妙的对策论 对这此有着很通俗易懂的解释。 对于某个 peer 的 Choking 算法 可以描述如下： Choking Algorithm：每 T 时间选择合适的 k 个 peer 进行 unchoke，选择的标准为过去 S 时间 peer 的下载速率； Optimistic Unchoking：每 nT 时间，随机选择一个 peer 进行 unchoke，以尝试发现更优质的 peer； Anti-snubbing：如果 mT 时间内没有从某个 peer 处获取到一个分片，则认为被 snubbed 了，对其进行 choke； Upload Only：当一个 peer 下载完成了，即成为了一个 seed，则只进行上传，不再下载。peer 会选择那些该 peer 对其有较高上传速率的 peer 进行上传。 实际实现中 T &#x3D; 10s, k &#x3D; 7, S &#x3D; 20s, n &#x3D; 3, m &#x3D; 6。 参考资料 Question About Canonical Peer Priority What do Flags and Reqs mean in uTorrent? - Super User","categories":[],"tags":[{"name":"BT","slug":"BT","permalink":"https://velih.de/tags/BT/"}]},{"title":"BT 增强建议之 Tracker","slug":"bt-tracker","date":"2018-08-26T16:00:00.000Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"2018/08/27/bt-tracker/","link":"","permalink":"https://velih.de/2018/08/27/bt-tracker/","excerpt":"","text":"本文是 BT 系列文章中的一篇，主要介绍种子文件结构与磁力链接的原理，有需要的话可以先阅读博文BT 增强建议之概述。 p2p 是 peer-to-peer 的缩写，为了让网络中的一个 peer 如何才能找到另外志同道合的 peer，Tracker 扮演着至关重要的月老作用。Tracker 是一个 HTTP 或者 UDP 服务器，作用是帮助 peer 找到其他拥有相同资源的 peer。 后来有了 DHT 网络之后，Tracker 的作用逐渐弱化，但是 Tracker 代表的这种中心化一定程度上相对 DHT 代表的去中心化效率还是比较高的。如同在概述前言中所说的一样，”过时“的技术某些情况下是会“复活”的，所以了解一下 Tracker 的工作原理并不是坏事。 HTTP TrackerHTTP Tracker 处理的来自 peer 的 GET 请求包括以下参数： info_hash：即需要下载的资源的 torrent 文件的 info 部分的 SHA1 值，或者磁力链接中的 xt 值。 peer_id：标示这个客户端的字符串，一般会包含客户端的版本信息以及随机数据（BEP20 - Peer ID Conventions）。 ip：可选，peer 的 IP。 port：可选，peer 监听的端口，一般是 6881。理论上 Tracker 应该需要对这个 ip 和 port 的组合进行 NAT 检查。 uploaded：该资源至今为止的上传字节数。 downloaded：该资源至今为止的下载字节数。 left：该资源剩余未完成下载的字节数。 event：可选，当前资源下载状态，可以是started，completed，stopped，每次发生状态变化时进行通告。 这个请求的回复是使用 bencode 编码的字典，有如下的键： failure reason：字符串，如果请求失败就会有这个键，表示失败的原因，另外 BEP31 - Failure Retry Extension 中定义了一个 retry in 字段来告知发起请求的 peer 重试间隔分钟数，或者永远不进行重试。如果成功就会有以下两个键。 interval：整型，告知 peer 之后进行 GET 请求的时间间隔，但是如果 peer 的状态发生变化或者需要从 Tracker 获取更多的 peer 就会不管这个时间间隔限制而直接重新请求。 peers：列表，每个子项都是字典，字典的键分别为 id，ip，port，分别与 GET 请求中的 peer_id，ip，port 含义一致，用于唯一确定每个其他 peer。 在 HTTP Tracker 的 URL 上进行适当的改造可以得到 Scrape URL（在 BEP48 - Tracker Protocol Extension: Scrape 中提出）。通过请求 Scrape URL 可以得到指定 info_hash 的当前 peers 的大致统计信息，包括处于活跃状态且已完成下载（complete）的 peer 数量、处于活跃状态但未完成下载（incomplete）的 peer 数量、曾经完成过下载（downloaded）的 peer 数量。支持同时请求多个 info_hash 。这些数据帮助 peer 决定是否应该执行 Tracker GET 请求，从而一定程度上减小 HTTP Tracker 服务器的压力。 而 UDP 本身比 HTTP 消耗更小些，且直接实现了 Scrape 请求。 UDP Tracker由于 HTTP 是基于 TCP 的，连接的打开和关闭会带来一定损耗。这种类似的发现服务使用 UDP 可以大大降低 Tracker 服务器的压力。BEP15 - UDP Tracker Protocol 提出了基于 UDP 的 Tracker 方案。 为了防止 UDP Flood 攻击，peer 与 UDP Tracker 的通信会首先通过 connect 动作进行类似 TCP 的三次握手，约定一个 connection_id，之后所有的动作都会使用这个 connection_id。announce 动作可以向 UDP Tracker 完成类似对 HTTP Tracker 的 GET 请求。使用 scrape 动作完成类似对 HTTP Tracker 的 Scrape 请求。如果出现错误，Tracker 会触发 error 动作。 GET 请求如果需要扩展参数只需要在请求的 URL 中增加相关的参数即可，在 UDP Tracker 中，实现这种增加参数的可扩展性则需要另外想办法。BEP41 - UDP Tracker Protocol Extensions 对于其实现方案举了一个这样的例子：一个 Tracker 服务器想要提高它的运行效率而限制其所能服务的 info_hash，他们考虑在在 peer 对 Tracker 的 URL 中加入一个 auth 字段，即该 info_hash 的签名，这样的情况下，Tracker 可以通过 Torrent 或者 Magnet 发布这个带有 info_hash 签名的 Tracker URL，peer 对该 Tracker 的请求时带上该签名，Tracker 就可以验证这个签名是否与 info_hash 相匹配以及这个 info_hash 是否在其服务的范围，从而实现限制。 比如 Tracker 在一个 Magnet 中附加的 tr 参数如下，auth 字段就是指的 info_hash 的示例签名： 1udp://tracker.example.com:80/?auth=0xA0B1C 实现 BEP41 方案后的客户端在对 UDP Tracker 发起 announce 请求动作时，会在包体尾部添加如下字节： 10x2, 0xC, &#x27;/&#x27;, &#x27;？&#x27;, &#x27;a&#x27;, &#x27;u&#x27;, &#x27;t&#x27;, &#x27;h&#x27;, &#x27;=&#x27;, &#x27;A&#x27;, &#x27;0&#x27;, &#x27;B&#x27;, &#x27;1&#x27;, &#x27;C&#x27;, 0x1, 0x1, 0x0 0x2 表示附加的选项类型是 URLData，这个选项会紧跟着两个参数，0xC 表示后面的内容长度为 12，后续的 12 字节就是 URLData 的内容；0x1 表示附加的选项类型是 NOP ,即填充字节，连续两个 0x1 表示填充两个字节，0x0 表示附加的选项类型是 EndOfOptions，即结束标志。最后三个选项不是必须的，这里加入他们只是说明有这些选项。 当然这里例子在 HTTP Tracker 中也容易实现，但是 UDP Tracker 有着更好的网络优化。 Tracker 相关增强方案BEP 中还有不少改进提案是针对 Tracker 的： Torrent 文件多 Tracker 支持Torrent 文件中 announce 字段只支持定义一个 Tracker，BEP12 - Multitracker Metadata Extension 中提出了让 Torrent 携带多个 Tracker 的方案。在 Torrent 文件根节点增加一个键 announce-list，这里简易的表示为 &#x2F;announce-list，之后在 Torrent 中增加键时也会简易地表示成这样。如果 Torrent 文件中有 announce-list，则会忽略 announce。announce-list 是一个列表，列表的每个子项是都是一个子项是 Tracker URL 的列表。之所以是一个子项为列表的列表的原因在 BEP12 中有详细的介绍与举例。此处不详细展开。 DNS 辅助纠正 Tracker 地址考虑到部分种子中的 Tracker 服务器可能变更服务端口或者讲 HTTP 服务改为 UDP 服务。BEP34 - DNS Tracker Preferences 一种基于 DNS 的解决方案。如果客户端发现自己所请求的 Tracker 没有相应，则可以查询相应域名的 DNS TXT 记录。如果有 TXT 记录是以 ”BITTORRENT“ 开头，则可以对这个 Tracker 地址进行纠正。这类 TXT 记录有以下几种类型： “BITTORRENT”：表明该主机不再运行任何 Tracker 服务。 “BITTORRENT DENY ALL”：和前一个一致，但是表意更加明显。 “BITTORRENT UDP:1337 TCP:80”：表示这个主机运行两个 Tracker 服务，分别时在 UDP 端口 1337 和 TCP 端口 80 上，且优先建议使用 UDP 端口。 以下是一个相关的 TXT 记录查询过程，这个主机在多个 UDP 端口上运行了 Tracker 服务： dig txtview raw12345678910111213141516171819&gt; dig 9.rarbg.to txt;; QUESTION SECTION:;9.rarbg.to. IN TXT;; ANSWER SECTION:9.rarbg.to. 5134 IN TXT &quot;BITTORRENT UDP:2710&quot;9.rarbg.to. 5134 IN TXT &quot;BITTORRENT UDP:2720&quot;9.rarbg.to. 5134 IN TXT &quot;BITTORRENT UDP:2730&quot;9.rarbg.to. 5134 IN TXT &quot;BITTORRENT UDP:2740&quot;9.rarbg.to. 5134 IN TXT &quot;BITTORRENT UDP:2750&quot;9.rarbg.to. 5134 IN TXT &quot;BITTORRENT UDP:2770&quot;9.rarbg.to. 5134 IN TXT &quot;BITTORRENT UDP:2780&quot;9.rarbg.to. 5134 IN TXT &quot;BITTORRENT UDP:2790&quot;9.rarbg.to. 5134 IN TXT &quot;BITTORRENT UDP:2800&quot;;; Query time: 207 msec;; SERVER: 1.1.1.1#53(1.1.1.1);; WHEN: Sat Aug 25 05:46:02 CST 2018;; MSG SIZE rcvd: 327 Tracker 返回公网 IPBEP24 - Tracker Returns External IP 通过在 GET 请求的返回中（UDP 也适用，下同）增加一个 external ip 字段来告知发起请求的 peer 自身的公网 IP。[Todo:客户端需要知道自己的公网 IP 做什么用暂时还不太清楚……ff]通过 peer 间的 yourip 扩展消息也可以获得自身的公网 IP。 Tracker 返回压缩 peer 列表BEP23 - Tracker Returns Compact Peer Lists 提出了一种压缩 GET 请求返回中 peers 字段值的方式以在一定程度上减小 Tracker 服务器的流量压力。这种方案的返回值中 peers 字段不再是一个列表，而是一个字符串，它由代表每个 peer 的六个字节（4 个字节用于 IP 地址，2 个字节用于端口）拼接而成。去除了 peer_id 这个字段，事实证明没有这个字段也没什么关系。Peer 通过在 GET 请求中增加 compact 参数来告知 Tracker 服务器自己更喜欢什么格式，0 为原始格式，1 为压缩格式。但是 Tracker 不一定会接受这个参数的建议，所以客户端仍然需要同时支持两种格式。 IPv6 支持BEP7 - IPv6 Tracker Extension 提供了优化 Tracker 对 IPv6 支持的方案。通过在 GET 请求中增加 ipv6 参数或者 ipv4 参数来告知 Tracker 服务器自己的相应 IP 版本的地址，如果在这两个字段中没有端口信息，那么将 port 字段作为端口。如果 Tracker compact 格式返回 peer 列表，那么它会在回复中增加 peers6 字段以返回使用 IPv6 的 peer，每个 peer 占用 18 字节。编码方式与 peers 字段一致。 参考资料","categories":[],"tags":[{"name":"BT","slug":"BT","permalink":"https://velih.de/tags/BT/"}]},{"title":"BT 增强建议之 Metadata：Torrent 与 Magnet","slug":"bt-metadata","date":"2018-08-26T14:00:00.000Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"2018/08/26/bt-metadata/","link":"","permalink":"https://velih.de/2018/08/26/bt-metadata/","excerpt":"","text":"本文是 BT 系列文章中的一篇，主要介绍 Tracker 服务器的工作原理，有需要的话可以先阅读博文BT 增强建议之概述。 在磁力链接出现前，BT 下载的第一步就是获取 Torrent（种子）文件。种子文件中包含了资源的最关键信息 —— Metadata（元数据）。Magnet（磁力链接）的引入则省去了获取种子文件这一步，但是仍然需要元数据，只是改为从 peer 处获取。有了元数据后，才能知道整个资源的概况，继而进行下载。 Torrent文件结构种子文件使用 bencode 进行编码，整个文件是一个字典。有下列主要的 key（value 中，整型值的单位均为字节，字符串默认使用 UTF-8 编码）： announce：字符串。tracker 的 URL 地址，仅能定义一个 Tracker。 info：字典。也就是资源的元数据： length：整型。如果有该字段，则代表种子为单文件种子，代表该文件的大小。 files：列表，子项类型为字典。如果有该字段，则代表种子为多文件种子。一个种子只能为单文件种子或者多文件种子，因此 files 字段和上述 length 字段只能选其中之一。files 字典的字段包括： length：整型。代表该文件的大小。 path：列表，子项类型为字符串。子目录名称列表，最后一项为文件名，因此该列表长度至少需要为 1。 name：字符串。如果是单个文件的种子，那么这个字段表示该文件的文件名，否则表示多个文件存储的根目录。 piece length：整型。为了方便传输与节点间交换数据，文件被分片，这个字段代表每个片段的大小，除了最后一片可能会被截断，这个值一般为 2 的幂次。 pieces：被编码成字符串，但是需要被解析成 SHA1 值。代表所有片段的 SHA1 值（每个值占用 160 比特）。 举例单文件种子这是一个单文件种子文件的 JSON 化结构示意，文件 debian-503-amd64-CD-1.iso 被分成大小为 256 KiB 的 $\\left(\\lceil\\frac{length}{piece length}\\rceil &#x3D; 2588\\right)$ 片。 12345678910&#123; &quot;announce&quot;: &quot;http://bttracker.debian.org:6969/announce&quot;, &quot;info&quot;: &#123; &quot;name&quot;: &quot;debian-503-amd64-CD-1.iso&quot;, &quot;piece length&quot;: 262144, &quot;length&quot;: 678301696, &quot;pieces&quot;: &lt;binary SHA1 hashes&gt; &#125;&#125; 多文件种子这个一个包含两个文件的种子的示例。相对于单文件种子，它使用了 files 字段取代了 length 字段。 1234567891011121314&#123; &quot;announce&quot;: &quot;http://tracker.site1.com/announce&quot;, &quot;info&quot;: &#123; &quot;name&quot;: &quot;directoryName&quot;, &quot;piece length&quot;: 262144, &quot;files&quot;: [ &#123;&quot;path&quot;: [&quot;111.txt&quot;], &quot;length&quot;: 111&#125;, &#123;&quot;path&quot;: [&quot;222.txt&quot;], &quot;length&quot;: 222&#125; ], &quot;pieces&quot;: &lt;binary SHA1 hashes&gt; &#125;&#125; MagnetMAGNET-URI Project提到 Magnet（磁力链接）大家都会想到 BT，但是磁力链接不是因为 BT 而诞生的，也不止用于 BT，事实上磁力链接的来自 MAGNET-URI Project 这个项目： MAGNET is a work-in-progress URI specification, and collection of standard practices&#x2F;implementing code to allow a website to seamlessly integrate with features made available by local utility programs. In one way, it could be thought of as a vendor- and project-neutral generalization of the “freenet:” and “ed2k:” URI-schemes used by the Freenet and EDonkey2000 peer-to-peer networks, respectively. Gordon Mohrmagnet-uri.sourceforge.net/magnet-draft-overview.txt 磁力链接是一个统一的规范，它希望这种 p2p 的链接都可以以按照这个规范展示，这样的话当用户在网页上点击磁力链接的时候，就可以磁力链接的参数（xt，下文会提及）“召唤”合适的客户端。它先被 eDonkey（电驴）推动，电驴链接理论上可以被转换成磁力链接。转换过程大致如下： 12ed2k://|file|&lt;name&gt;|&lt;file-size&gt;|&lt;ed2k-hash&gt;|/magnet:?xt=urn:ed2k:&lt;ed2k-hash&gt;&amp;xl=&lt;file-size&gt;&amp;dn=&lt;name&gt; 然而，这个 MAGNET-URI Project 后来应该没有被推动下去，导致甚至连电驴自己的客户端都没有支持 ed2k 的 magnet 格式。直到后来在 BT 中大放异彩，导致现在狭义上的磁力链接就是指 BT 中使用的磁力链接。 BT 中的磁力链接BT 中的磁力链接大概有这两种格式： 12v1: magnet:?xt=urn:btih:&lt;info-hash&gt;&amp;dn=&lt;name&gt;&amp;tr=&lt;tracker-url&gt;&amp;x.pe=&lt;peer-address&gt;v2: magnet:?xt=urn:btmh:&lt;tagged-info-hash&gt;&amp;dn=&lt;name&gt;&amp;tr=&lt;tracker-url&gt;&amp;x.pe=&lt;peer-address&gt; 根据 URL 的定义，magnet 前缀表示这个链接是磁力链接，？ 后表示为 GET 模式查询参数列表，参数使用 &amp; 符号隔开。BT 磁力链接的参数如下: xt：表示包含文件散列函数值的 URN，这是唯一一个必选参数，可能的 URN 类型有： btih：表示 Torrent 文件 info 部分的 SHA1 值，可以是 Hex 编码或者 Base32 编码形式。 btmh：表示 Torrent 文件 info 部分的 HEX 编码 multihash 值，multihash 是由创造 IPFS 的 Protocal Lab 的项目，用于编码多种 hash 函数的 hash 结果。btmh 可以和 btih 同时存在，这个应该和 SHA1 被破解相关——有 btmh 中使用其他 hash 函数得到的 hash 值加上原来的 SHA1 值就可以做到兼容，同时检测碰撞。 dn：表示建议显示的文件名。 tr：表示 Tracker的 URL，如果有多个 Tracker,则会有多个 tr 参数。 x.pe：表示 Peer 的，格式为 hostname:port，ipv4-literal:port 或者 [ipv6-literal]:port，这些 peer 可以被添加到 peer 列表中用于获取元数据以及后续的文件片段获取。实际上 Magnet 链接中定义了一个通用的参数 xt 用于指定类似 x.pe 所表示的 p2p 连接，但是由于没有合适的 URI 标识符分配给 BT（比如电驴有 ed2k），所以 BT 使用了这个参数，而不是 xt。 so：定义在 BEP53 - Magnet URI extension - Select specific file indices for download 中，用于指定下载特定文件，比如 so=0,2,4,6-8 表示下载 files 列表中索引为 0,2,4,6,7,8 的这六个文件。 有了磁力链接，客户端就可以向 peer 请求 Torrent 文件的 info 部分了，获取完成后就相当于拥有了 Torrent 文件，也就是有了完整的元数据，继而可以下载资源。 参考资料 Wikipedia - Torrent file Online Torrent File Decoder Online Magnet Link to Torrent File converter Wikipedia - Magnet URI scheme Stack Overflow - What is the difference between URI, URL and URN?","categories":[],"tags":[{"name":"BT","slug":"BT","permalink":"https://velih.de/tags/BT/"}]},{"title":"BT 增强建议之概述","slug":"bt-overview","date":"2018-08-26T09:00:00.000Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"2018/08/26/bt-overview/","link":"","permalink":"https://velih.de/2018/08/26/bt-overview/","excerpt":"","text":"在版权意识愈渐加强的今天，BT 流量占全球流量的比重不断下降，这种 p2p 技术的应用在逐渐衰落。但是请记住 “技术无罪”——纵然据说爱因斯坦也曾对“把原子弹送到了疯子手里”感到后悔。 “个体重复系统发育”：技术的变化会导致某些思想过时并迅速消失，这种情形经常发生。但是，技术的另一种变化还可能使某些思想再次复活。 Andrew Tanenbaum现代操作系统 总目录下面进入正题。一个使用 BT 进行下载的过程可以简短地描述如下： 当你获取了一个磁力链接或者种子文件，使用 BT 客户端打开文件确认下载后，客户端就成为了一个 peer，客户端通过连接 Tracker 服务器或者 DHT 网络寻找到其他拥有所需要文件分片的 peer，从这些 peer 中下载资源分片，同时客户端也上传数据给其他来索要自己所拥有分片的 peer，以此反复，直到下载完成。 搞清楚这个下载过程中到底发生了什么事情就是记录这个系列文章的目的所在。整个学习过程以 BT 增强建议 BEP 为主要参考，同时适当参考 BT 的一个 Java 版本实现源码。 整个系列分为： BT 增强建议之概述：主要介绍 BEP 与 Bencode 编码 BT 增强建议之 Metadata：Torrent 与 Magnet：Torrent 种子文件结构与 Magnet 磁力链接的原理 BT 增强建议之 Tracker：作为 Peer 间桥梁的 Tracker 服务器的工作原理 BT 增强建议之 Peer： Peer 间的通信的过程以及以牙还牙策略 DHT：使得 BT 网络脱离 Tracker，实现完全去中心化 DHT 网络之 Kademlia 算法 BT 增强建议之 DHT BT 增强建议之进阶改进方案：BEP 中提出的一些进阶改进方案 用于保证 BT 高速下载时其他应用低时延网络通信的传输层协议 µTP 已在独立的博文 µTP 协议 —— 对 BEP29 的简要理解 中介绍。 这个系列将尽可能涵盖所有的 BEP。下面的表格显示了章节与 BEP 的引用关系，因此在每篇文章的参考资料中将不在列举相关 BEP。 BEPBEP（BitTorrent Enhancement Proposal）是 BitTorrent 社区仿照 PEP（Python Enhancement Proposal）来改进 BitTorrent 的技术文档，可以视为一种开发方式。一个规范的 BEP 被提出后可能经历如下的几个过程，但是目前只有 Final，Accepted，Draft，Deferred 四种状态的 BEP。同时 Bittorrent 和 Python 还有个相似的共同点，他们的最初设计者都曾经是自己项目的终身仁慈独裁者（Benevolent Dictator For Life，BDFL）。例如 BEP1000 - Pending Standards Track Documents 中所说的这个不存在的 BEP13 - Protocol Encryption 一直没能成为正式的 BEP 的原因就和 BitTorrent 的作者 Bram Cohen 反对 BT 流量加密相关。 2018年7月，Guido van Rossum 宣布不再担任 Python 社区的 BDFL。 2018年8月，在自己一手创办的公司被波场收购后，Bram Cohen 表示自己已经离开了 Bittorrent，目前在做数字货币相关的工作。 Bencodebencode 是 BT 协议在传输数据过程中广泛采用的一种编码格式。主要支持以下四种数据类型的编码： String：十进制字符串占用字节数 + ‘:’ + 字符串。例如 “spam”会被编码成“4:spam” Integer：’i’ + 十进制整形数字 + ‘e’。除 0 之外，不能以 0 开头。例如：i3e、i-3e List：使用字符 ‘l’ 和 ‘e’ 进行界定，中间是其他元素。例如 l4:spami:-42ee 代表列表 [spam, -42] Dictionary：使用字符 ‘d’ 和 ‘e’ 进行界定，中间是每个键值对元素,而且所有键为字符串类型并按字典顺序排列。例如 d3:cow3:moo4:spam4:eggse 代表字典 {cow: moo, spam: eggs} 参考资料 BEP 的 Github 地址 Wikipedia - BitTorrent protocol encryption BT流量识别技术的研究 谈eD2k和电驴的兴衰 Slashdot - BitTorrent Founder Bram Cohen Has Left the Company","categories":[],"tags":[{"name":"BT","slug":"BT","permalink":"https://velih.de/tags/BT/"}]},{"title":"µTP 协议 —— 对 BEP29 的简要理解","slug":"µtp","date":"2018-08-04T09:05:00.000Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"2018/08/04/µtp/","link":"","permalink":"https://velih.de/2018/08/04/%C2%B5tp/","excerpt":"","text":"在 TCP 温故知新中回顾了 TCP，而这篇文章主要讲用于 BT 网络的基于 UDP 的运输层协议 µTP，同时顺便回顾 UDP。下面的内容更多是基于对 BEP29 的理解。 名字探究 µTP 的主要文档 BEP29 的创建于 2009 年，姑且认为这也是设计完成的大致时间，µTP 在 uTorrent 的 1.8 中首次加入（2009 年）这个事实也佐证了这点。它的设计者包括： Ludvig Strigeus（μTorrent 作者，BitTorrent 公司 2006 年收购 μTorrent，目前在 Spotify 工作） Greg Hazel Stanislav Shalunov（µTP 中的拥塞算法 LEDBAT 主要作者，后来还创造了可以脱离现有蜂窝网络使用的 IM 应用 FireChat —— 它被多次用到”占中”这样的公民运动中） Arvid Norberg（libtorrent 开发者，目前在 Blockstream 工作） Bram Cohen（BitTorrent 作者） 设计原因 在过去多年前，如果使用 BT 进行下载热门资源的话可以感受到到速度飞快，但是同时带来的问题就是如果想要同时看在线视频就会带来卡顿。问题在于 DSL 和 modem 的往往有一个和它们的发送速率不成比例的发送缓冲区，不成比例到可以容纳几秒钟的数据量。而 BT 往往会与许多 peer 建立 TCP 连接，在 TCP 将带宽均匀地分配到每个连接的前提下，BT 就占用了较多的带宽，但是其他诸如浏览网页、即时通讯这些场景的优先级实际上应该要比 BT 传输更高些，但是因为 BT 和 物理层的这种特性导致了其他服务有延迟，影响了使用 BT 时其他服务的体验。 µTP 通过将 modem 的缓冲队列的大小作为一个控制因子来调整发送速率，当队列过大时，将会放慢发送速度。这种策略使得 BT 在没有竞争的情况下可以充分利用上传带宽，在有大量其他流量的情况下则放慢发送速率。 上述时 Bittorrent 文档中的说法，但是实际上 µTP 是基于包的延时的，而不是基于队列大小的。而且 µTP 具体对拥塞算法 LEDBAT 的实现有与在 IETF 互联网草案 RFC 6817 中的描述有所区别（这里的实现指 C++ 版本的 libutp，各个版本的实现不完全一致）。 UDP 和 µTP 首部 为了方便与 TCP 做简单的对比，把 UDP 的首部（前四个字节）与 µTP 的首部（剩余部分）放在了一个图中示意。从 UDP 首部字段数量皆可以发现 UDP 协议相对 TCP 协议是如此简单，以至于我们w可以将 TCP 协议看成是类似 µTP 协议一样的基于 UDP 协议的运输层协议。 UDP 首部 源 &#x2F; 目端口（Source &#x2F; Destination port）：用于确认通信进程双方。源端口是可选的，如果设置了源端口，则接收方将其视为回复端口。如果不需要回复就不需要设置； 总长度（Length）：定义了 UDP 用户数据报的总长度，包括首部和数据。TCP 首部中是没有所谓“报文段总长度”的字段的，长度可以通过 IP 层的长度减去 IP 首部长度计算所得，所以一定程度上时冗余的，可以参考 Stack OverFlow 上的相关讨论； 校验和（Checksum）：用于对整个用户数据报的校验，通过 IP 位首部与和用户数据报计算得到； 可见 UDP 首部的这些字段在理论上可以是 TCP 首部字段的子集。因此我们可以粗略地讲 TCP 是基于 UDP 的传输层协议。UDP 服务是一个“尽力而为”的服务，它没有流量控制，只能通过校验和进行差错控制，丢包不会知晓，也不会重传，也没有拥塞控制。 µTP 首部 version：版本号，现在为 1，还有一个原始版本号 0 存在 connection_id：用于标记连接。这个字段是必须的么?； timestamp_microseconds：发送数据包的时间，和计算延迟时间，rtt 相关； timestamp_difference_microseconds：当前时间和上一次收到的包中的 timestamp_microseconds 之差； wnd_size：表示另一端建议的窗口大小，相当于 TCP 中的接收窗口； extension：代表第一个扩展的类型，0 表示没有扩展，1 表示 Selective ACKs（选择确认扩展）。 type：数据包的类型。可以有： ST_DATA &#x3D; 0：承载有效数据的数据包； ST_FIN &#x3D; 1：终止连接。是通信单方发送的最后一个包，类似于 TCP 的 FIN 标记。但是发送 ST_FIN 的一方还是会等待可能丢失或者失序到达的包，即使收到了对方的 ST_FIN 包； ST_STATE &#x3D; 2：用于报告状态，传输一个没有数据的 ACK。和 TCP 中未携带数据的 ACK 包一样，它不消耗序号； ST_RESET &#x3D; 3：强制重置连接。类似 TCP 的 RST 标记。 ST_SYN &#x3D; 4：发起连接。类似 TCP 的 SYN 标记。发起方会随机一个 connection_id ID 用于给接收方后续的回复使用。然后发起方之后的包中传输的 connection_id 为 ID + 1； seq_nr：表示这个数据包的序号。注意和 TCP 中代表字号号的序号有所不同。初始序号同样也是随机生成。 ack_nr：表示最后收到的数据包的seq_nr； 连接过程 图示为一次在 qBittorrent 中对一个种子开始 BT 下载二十秒左右后停止下载时，与其中一个 peer 的交互过程。 丢包和丢包处理有一个和窗口相关的变量需要先进行说明： max_window：定义了未被确认的字节的上限，相当与 TCP 中的拥塞窗口； wnd_size：同首部中的 wnd_size，和 TCP 中相同，实际的发送窗口大小为 min(max_windows, wnd_size)； cur_window：表示当前未被确认的字节的数量； 当需要发送数据包时，仅当 cur_window + packet_size &lt;= min(max_windows, wnd_size) 成立，这个数据包才能被发送。 当序号为 seq_nr - cur_window 的数据包（发送队列中年龄最大的未被确认的数据包，下一个理论上需要被确认的就是它）没有被确认，但是已经有至少三个序号大于它的数据包通过 Selective ACK 被确认，那么这个数据包被认为是丢失了。 如果 ack_nr + 1 这个包已经发送了，而收到三个重复的 ack_nr 的 ACK，那么 ack_nr + 1这个包被认为是丢失了。 如果检测到丢包，那么拥塞窗口 max_window 大小减半。 超时µTP 的超时计时和 TCP 中 RTO 计时器不太一样，RTO 计时器主要用于对 ACK 的计时，但是 µTP 的计时器则是指接收任何数据包超时时间，如果超时时间内没有任何数据包到达则超时，如果超时，则 packet_size 和 max_window 将都会被设置为最小数据包大小（150 字节）。 （疑问？：packet_size 的调整策略是什么） 超时时间的计算每当收到一个数据包的 ACK，就会更新往返时间（不考虑重传的包），往返时间用于计算超时时间。先说明下相关的变量： rtt：往返时间； rtt_var：往返时间差异； packet_rtt：当前收到 ACK 对应的包的往返时间，即当前时间减去这个包发送时的时间，即首部中的 timestamp 字段记录的值。 timeout：超时时间； 通过以下公式更新 rtt： 123delta = rtt - packet_rttrtt_var += (abs(delta) - rtt_var) / 4;rtt += (packet_rtt - rtt) / 8; 正常情况下通过 rtt 计算得到 timeout（第一次由于没有 rtt，timeout 使用初始值 1000 ms，）： 1timeout = max(rtt + rtt_var * 4, 500); 如果遇到超时，则 timeout 翻倍。 拥塞控制在丢包和超时部分已经涉及到拥塞窗口的调整了。µTP 的拥塞控制可以理解为一种基于延迟的负反馈拥塞控制，通过延迟的变化控制窗口大小的变化，达到拥塞控制目的。计算窗口大小过程中的相关常量和变量的定义如下： CONTROL_TARGET：µTP 所能接收的上行缓冲延迟，现在是 100 ms； base_delay：两分钟内的收到的数据包的最低延迟； our_delay：当前的数据包延迟； off_target：实际延迟和目标延迟的差距。即CONTROL_TARGET - our_delay； outstanding_packet：已经被发送但是未被确认的数据包； 具体计算过程如下： 12345delay_factor = off_target / CCONTROL_TARGET;window_factor = outstanding_packet / max_window;scaled_gain = MAX_CWND_INCREASE_PACKETS_PER_RTT * delay_factor * window_factor;max_window += scaled_gain; 当 off_target &gt; 0 时，当前包的延迟时间比设定的还小一些，那么窗口会缩小，发送速率就会放慢；反之，窗口增加，速率加快。 参考资料 BitTorrent Enhancement Proposal 29 - µTorrent transport protocol RFC 6817 - Low Extra Delay Background Transport (LEDBAT) Calvin’s Marbles - libutp源码简析","categories":[],"tags":[{"name":"BT","slug":"BT","permalink":"https://velih.de/tags/BT/"},{"name":"Network","slug":"Network","permalink":"https://velih.de/tags/Network/"}]},{"title":"TCP 温故知新","slug":"tcp","date":"2018-07-19T16:00:00.000Z","updated":"2020-07-18T07:11:02.000Z","comments":true,"path":"2018/07/20/tcp/","link":"","permalink":"https://velih.de/2018/07/20/tcp/","excerpt":"生产环境遇到些网络问题，知对 TCP 协议还是有些生疏，在此复习记录。","text":"生产环境遇到些网络问题，知对 TCP 协议还是有些生疏，在此复习记录。 协议报文段格式TCP 协议报文段主要由首部（Header） 与数据（Data） 两部分组成。在计算校验和是还会加上虚拟的伪首部。此处主要说明首部的组成。TCP 在网络模型中属于运输层，用于提供进程与进程间的字节流通信服务，因此需要源 &#x2F; 目端口（Source &#x2F; Destination port） 以确定通信双方进程。使用序号（Sequence number，seq） 表明本报文段第一个数据字节的编号，初始序号由双方在 TCP 连接建立时随机生成。使用确认号（Acknowledgment number，ack） 表示接受方期望从发送方接受的字节编号。数据偏移量（Data Offset） 顾名思义报文段数据开始字节处的偏移量，即 TCP header 的长度，由于选项的存在，首部长度的范围是 20～60 Bytes，但是该字段只有 4 Bits，因此该字段指出首部长度有多少个 4 Bytes。接下来的 3 Bits 被保留（Reserved） 。随后会讨论 9 个标志位（Flags）。窗口大小（Window Size） 定义了接受方的接受窗口大小，由接受方决定，然后告知发送方。在计算校验和（Checksum） 时需要加上伪首部，伪首部的内容包括源目 IP 地址，TCP 报文段长度等。如果 URG 标志位被设定了，那紧急指针（Urgent pointer） 用于指示紧急数据最后一个字节在报文段数据部分中的偏移量。最后的 40 Bytes 留给选项（Options）。 Flags URG：为 1 表示数据中有紧急数据。这个标记比较少见，可以找到的一些应用有：FTP，Telnet； ACK：为 1 表示确认号字段有效； PSH：为 1 表示有推送数据，这个字段主要完成两个功能：发送方应用层提醒 TCP 需要立即发送数据；接受方 TCP 需要将收到的数据立即提交给应用层； RST：为 1 表示出现严重差错。可能需要重现创建 TCP 连接。还可以用于拒绝非法的报文段和拒绝连接请求； SYN：为 1 表示这是连接请求或是连接接受请求，用于创建连接和使顺序号同步； FIN：为 1 表示发送方没有数据要传输了，要求释放连接； 阶段TCP 连接可以分为三个阶段：建立连接，传输数据，终止连接。可以用一个有限状态机表示：下面结合一个用 Rust 写的 Echo Server 与抓包工具 Wireshark 来演示这三个过程。代码如下： Echo Serverview raw1234567891011121314151617181920212223242526272829303132333435363738use std::net::{TcpListener, TcpStream};use std::thread;use std::io::Read;use std::io::Write;pub fn main() { let listener = TcpListener::bind(&quot;::1:9999&quot;).unwrap(); for stream in listener.incoming() { match stream { Ok(stream) =&gt; { thread::spawn(|| { handle_client(stream); }); } Err(_) =&gt; { println!(&quot;Error&quot;); } } }}fn handle_client(mut stream: TcpStream) { loop { let mut read = [0; 1024]; match stream.read(&amp;mut read) { Ok(n) =&gt; { if n == 0 { break; } stream.write(&amp;read[0..n]).unwrap(); } Err(err) =&gt; { panic!(err); } } }} Echo Server 启动之后使用 telnet 工具连接至服务器，与服务器交互两次，即回显两次字符串，然后从退出 telnet。抓到的包截图如下： 在下面的表述中，客户端表示在这一阶段先发起请求的一方，服务端表示在这一阶段先接受请求的一方。在 TCP 中，服务端和客户端之间可以进行双向通信，他们可以理解为对等的，在连接建立后，实际上没有服务端和客户端的区别。 建立连接前三个包代表了建立连接的过程，分为三步，大多数情况下就如同抓包所示，称为三次握手（3-way handshake）。图 2 中还画出了双方同时打开连接与连接复位这两种比较少见的情景。 客户端发送给服务端一个 SYN 报文，用以告知服务端初始序号为 0（此处的 Seq 经过 Wireshark 的处理，变成了相对值），然后客户端进入 SYN_SENT 状态。 服务器收到 1 中报文后，发给客户端一个 SYN + ACK 报文，告知客户端初始序号为 0，且接下来期望从客户端收到的序号 Ack 为 1。服务端进入 SYN_RCVD 状态。可见 SYN 报文消耗了一个序号。 客户端给服务端回复 ACK 报文，进入 ESTABLISHED 状态，服务端在收到这个报文连接建立完成。可见 ACK 报文在不携带数据的情况下不消耗序号。 传输数据图 3 中 P17～P20 与 P72～P74 分别展示了两次交互过程。以第二次交互为例： P72 中，客户端将想要回显字符串发送给服务端，告知服务器这个报文段的数据首字节序号为 8，并且希望从服务端收到的下一个序号是 8。 服务端将回显的字符串与对 P72 的确认一起发送给客户端。P72 的长度为 9，因此报文段中数据的序号范围为 [8,17)，因此在 P73 中 Ack 为 17。 客户端确认 P73。 需要注意的是，这两次交互有所区别：第一次交互时，服务端先返回了一个确认给客户端，然后再额外发送一条携带回显内容的报文段。但是第二次交互时省略了单独的确认。 终止连接根据“广大博主”写作的文章与问答网站的讨论以及各种面经，基本一致认为，终止连接通常被成为四次挥手（4-way termination），即通信双方会有四次交互，但是如图 2 所示，也存在三次挥手的可能性，且我本地的多次抓包也如图 3 一致——均为三次挥手，且无一例外。下面对四次挥手进行分析，三次挥手可以被其概括： 客户端发起终止连接，发出一个 FIN 报文给服务端，假设该报文的 Seq 为 X，Ack 为 Y。然后客户端进入 FIN_WAIT1 状态，除重传 FIN 报文与发送 ACK 确认之外，不再发送应用数据给服务端。 服务端收到 FIN 报文段后先回复一个 ACK 报文，进入 CLOSE_WAIT 状态，该报文的 Seq 为 Y，Ack 为 X+1。 客户端在收到 FIN 的 ACK 之后进入 FIN_WAIT2 状态。此时服务端还可以继续将未发送完的应用数据发送给客户端。 服务端发送完数据后发送一条 FIN 报文，进入 LAST_ACK 状态，该报文的 Seq 为 Y+K，Ack 为 X+1。 客户端收到服务端的 FIN 后，发送最后的 ACK 报文给服务器，然后进入 TIME_WAIT 状态。如果在 2MSL（最大报文段寿命，通常为30～60s）后客户端没再次收到 FIN 报文，则进入 CLOSED 状态，否则重发 ACK 报文进行重试。 服务端在收到 ACK 报文后进入 CLOSED 状态。 三次挥手表现为步骤 2～4 合并为一步，即同时发送对客户端 FIN 的 ACK 报文与 服务端自己的 FIN 报文。这条 FIN+ACK 报文的 Seq 为 Y，Ack 为 X+1。 至于在什么情景下出现三次握手或者四次握手，大多数的观点认为先收到 FIN 报文的一方还需要向上层应用询问是否仍然有数据需要发送，因为要等待上层的回复，所以“为何不早点把对 FIN 的 ACK 发出去呢？！”而且立即 Ack 能防止对方重传 FIN。但是有没有存在不需要询问上层或者不需要立即回复 Ack 的可能，就如同传输数据出现的情况一样，这些应该与 TCP 的具体实现相关，目前能力与精力有限，还有待对 Linux TCP 实现源码进行阅读。 滑动窗口滑动窗口是 TCP 中用于实现诸如 ACK 确认、流量控制、拥塞控制的承载结构。如图所示:先将 TCP 看作是简单的单向通信，则发送方有一个发送窗口，接收方有一个接收窗口。正如图中所示，传输的是报文段，窗口大小的单位是字节。两个窗口中白色区域为空白位置，等待被应用层或者网络层填满；灰色区域是已经发送但是还没有接受到确认的字节；粉色区域在发送窗口中表示等待被发送的字节，在接收窗口汇总表示等待交付的字节。发送窗口的大小为接受方通过首部中窗口大小字段告知的接收窗口大小 rwnd 和之后会讲到的拥塞窗口的大小 cwnd 两者中的较小值，即 min(rwnd , cwnd)。接收窗口大小是接收方可用缓存空间大小，为 rwnd = 缓存大小 - 准备交付的字节数。 流量控制流量控制用于平衡生产者产生数据与消费者消耗数据的速度。TCP 中的流量控制实现的主要途径是不断调整发送窗口的大小实现。发送 TCP 一旦发现发送发送窗口满了就会对发送进程进行反馈。接受方根据之前从发送方收到的数据量和服务器已经消耗的数据量得到自身接收窗口的大小，将这个值告知发送方，发送方根据收到的窗口大小调整自身窗口大小。还有正对通信双方产生数据小或者消费数据慢产生糊涂窗口综合征时，有其他方面的流量控制。当发送方数据量相对于首部小很多的报文很多时，可以使用 Nagle 算法减少这种小报文量。当接受方消耗数据很慢时，每次告知发送方的窗口大小会比较小，也可能产生很多小报文，此时可以使用 Clark 解决方法或者进行推迟确认。关于这几种方案的分析及具体的使用情景可以参考 dog520 大神的这篇文章——再次谈谈 TCP 的 Nagle 算法与 TCP_CORK 选项。 差错控制差错控制用于描述 TCP 在发送或者接受到报文段发生异常时的行为，其主要表现在如下几个方面： 校验和：在接受方，如果收到的报文段未通过校验和校验，则立即丢弃，反之，则通过确认规则（下面马上提及）进行确认； 重传：在发送方，如果一个报文段重传超时计时器（RTO）超时，即在 RTO 时间之后仍然未收到 ACK，则立即重传未被确认的最小的报文段；如果收到四个相同的的 ACK ，则立即重传下一个报文段； 确认规则 当接受方向发送方发送数据报文段时，必须捎带 ACK； 当接受方没有数据要发送时，但是收到一个按序到达的报文，同时前一个报文段也已经确认过了，那么接收方就推迟发送确认报文段，直到另一个报文段到达或者延迟一段时间以减少 ACK 报文量； 当所期望的报文段到达，且前一个按序到达的报文还未被确认，则立即 ACK（ACK 的序号仍然是下一个正常期望的序号，下同）； 当序号比期望的大的报文段（失序报文段）到达，则立即 ACK，且存储该报文段； 当丢失的报文段到达时，则立即 ACK； 当重复的报文段到达时，则丢弃，且立即 ACK； 拥塞控制拥塞控制是为了避免因为网络受限导致网络不能按照发送方产生的数据的速度将报文段交付给接受方。与流量控制考察的对象是通信双方不同，拥塞控制的考察对象是通信双方间的网络。拥塞控制的方式表现在发送方的拥塞窗口的变化，从而控制发送方的数据发送快慢。TCP 最初使用的拥塞策略称为 TCP Tahoe and Reno。 Tahoe and Reno这个拥塞策略主要分为慢启动、拥塞避免、拥塞检测三个阶段。 慢启动假设接收窗口大小远大于拥塞窗口，且不考虑延迟确认。拥塞窗口大小从一个最大报文长度 MSS （连接建立时通过 TCP 选项告知）开始，之后每当有一个报文段被确认，拥塞窗口就增大一个 MSS。在这样的策略下，慢启动阶段的拥塞窗口大小呈指数增长。即从 1 -&gt; 2 -&gt; 4 -&gt; 8。当到达慢启动门限时，就进入拥塞避免阶段。 拥塞避免拥塞避免阶段拥塞窗口大小继续增加，但是速度放慢，改成当整个窗口大小的报文段都被确认后，窗口大小才增加一个 MSS。表现为 8 -&gt; 9 -&gt; 10 -&gt; 11，呈现线性增长。 拥塞检测如果拥塞发生了，因为发生在网络中路由器出现丢包现象，在接受方处的表现为出现以下两种情况，并做出对应的反应： RTO 计时器超时。这说明网络拥塞的可能性较大，TCP 做出较强烈的反应： a. 把慢启动门限值调整为当前窗口大小的一半； b. 把 cwnd 重新设置为 1 MSS； c. 重新进入慢启动阶段； 收到四个相同的 ACK；说明出现拥塞的可能性较小，但是出现了丢包，TCP 做出较弱的反应，Reno 算法表现为快恢复（Fast recovery）： a. 把慢启动门限值调整为当前窗口大小的一半； b. 把 cwnd 重新设置为慢启动门限值； c. 重新进入拥塞避免阶段； 图示如下：Tahoe 算法与 Reno （Tahoe 的改进版本）的区别在于收到四个相同 ACK 时，Tahoe 算法的策略和 RTO 计时器超时时一致。 其他拥塞策略拥塞控制策略只需要在发送方实现即可，不需要接受方的参与，因此可以仅在发送方部署一套算法。现在 TCP 网络上的算法也在不断改进，涌现出诸如 TCP CUBIC、TCP BBR 这样的算法。 TCP 中的计时器重传计时器重传计时器的超时时间为 RTO，RTO 主要根据测量所得的报文段往返时间 RTTm计算而来，计算过程如下： 首先计算平滑 RTT，即RTTs： 12RTTs = RTTm // 第一次测量RTTs = (1 - α)RTTs + αRTTm // 随后的测量 然后计算 RTT 的偏差 RTTd： 12RTTd = RTTm / 2 // 第一次测量RTTd = (1 - β)RTTd + β|RTTs - RTTm| // 随后的测量 重传超时 RTO 的计算如下： 12RTO = 6s // 原始值RTO = RTTs + 4RTTd // 第一次测量后 任意时刻只有一个 RTT 在进行测量，当重传发生时会影响 RTT 的测量，根据 Karn 算法，TCP 忽略重传报文的 RTT，而对于重传的报文，RTO 值为原报文的两倍，如果发生第二次重传，则为四倍，以此类推。 持续计时器当接受方发送窗口值为 0 的报文段之后，后来因为接收窗口增加需要通告接窗口为非 0，但是通告的 ACK 丢失，这会导致发送方一直等待非 0 窗口通告，导致死锁。持续计时器就是接受方在收到 0 窗口通告后启用，如果超时则发送探测报文，促使接受方重传 ACK。 Keep-Alive 计时器Keep-Alive 计时器用于防止 TCP 连接长时间空闲，每当收到对方的报文段，这个计时器就复位。这个计时器的超时时间通常可以通过接口进行设定。 Time-Wait 计时器Time-Wait 计时器对最后的 FIN 进行确认时启动的超时计时器。如果在时常为 2MSL 的计时器超时前再次收到 FIN，则重传 ACK，否则连接彻底关闭。 传输层的未来QIUC 了解一下下？ 参考资料 TCP&#x2F;IP 协议族 第四版 Andrew S.Tanenbaum - Computer Networks 5th WikiPedia - Transmission Control Protocol WikiPedia - TCP congestion control TCP Flags: PSH and URG KTH - Internetworking Lecture 4 一个 TCP FIN_WAIT2 状态细节引发的感慨","categories":[],"tags":[{"name":"Network","slug":"Network","permalink":"https://velih.de/tags/Network/"}]}],"categories":[],"tags":[{"name":"考","slug":"考","permalink":"https://velih.de/tags/%E8%80%83/"},{"name":"BT","slug":"BT","permalink":"https://velih.de/tags/BT/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://velih.de/tags/Algorithm/"},{"name":"翻译","slug":"翻译","permalink":"https://velih.de/tags/%E7%BF%BB%E8%AF%91/"},{"name":"IPFS","slug":"IPFS","permalink":"https://velih.de/tags/IPFS/"},{"name":"Network","slug":"Network","permalink":"https://velih.de/tags/Network/"}]}